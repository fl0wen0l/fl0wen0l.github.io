[{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Internship Worklog ‚Äî Weekly Summary This worklog documents a 12‚Äëweek internship program with weekly notes on objectives, tasks, outcomes and useful commands. Use each weekly link to review detailed notes, scripts, CLI examples and verification steps.\nHow to use this page\nThe table below provides a compact summary for each week (title, dates, short highlights and a link to the detailed weekly page). Click a weekly link to view the detailed tasks, code snippets, and verification steps that were executed. High-level summary\nDuration: 12 weeks Focus areas: Cloud fundamentals, networking, compute, storage, databases, monitoring, scaling, hybrid connectivity, migration and production-like deployments. Weekly overview\nWeek Date Topic Highlights Link Week 1 2025-09-11 Getting started ‚Äî EC2 \u0026amp; CLI Created AWS account, configured CLI, launched EC2, attached EBS, practiced SSH and basic CLI commands. Week 1 Week 2 2025-09-23 VPC \u0026amp; Networking Designed and deployed a secure VPC (public + private), configured SGs/NACLs, implemented Site‚Äëto‚ÄëSite VPN and automated via IaC. Week 2 Week 3 2025-09-30 EC2 Operations \u0026amp; App Deploy Launched Linux and Windows instances, created AMIs/snapshots, deployed sample apps, and validated recovery and SSM access. Week 3 Week 4 2025-10-07 IAM Roles \u0026amp; Instance Profiles Implemented role-based access for EC2 instances, rotated test keys, and applied least-privilege policies. Week 4 Week 5 2025-10-14 Cloud9 \u0026amp; S3 Static Hosting Used Cloud9 as a development environment; hosted static site on S3, configured CloudFront and bucket versioning. Week 5 Week 6 2025-10-21 RDS \u0026amp; Relational Databases Launched RDS instances, configured Multi-AZ and backups, and validated snapshot/restore procedures. Week 6 Week 7 2025-10-28 AWS Lightsail Deployed WordPress/PrestaShop/Akaunting; used Lightsail DBs, snapshots and cost optimization best practices. Week 7 Week 8 2025-11-05 Auto Scaling \u0026amp; Load Balancer Built Launch Template, Application Load Balancer and Auto Scaling Group; validated scaling and health checks. Week 8 Week 9 2025-11-12 Observability ‚Äî CloudWatch Created metrics, logs, alarms and dashboards; used CloudWatch Logs Insights and created example dashboards. Week 9 Week 10 2025-11-19 Hybrid DNS (Route 53) \u0026amp; AWS CLI Implemented Route 53 Resolver endpoints and learned CLI automation for resource management and DNS testing. Week 10 Week 11 2025-11-26 Migration \u0026amp; CLI Automation Used AWS CLI to collect inventory, staged artifacts, ran VM import/export tests and explored DMS migration steps. Week 11 Week 12 2025-12-04 Production-style WordPress Deployment Deployed WordPress with EC2 + RDS + ALB + ASG and verified backups, CloudFront and cleanups. Week 12 Key outcomes \u0026amp; skills\nGained hands-on experience across core AWS service stack and deployment patterns. Improved confidence with AWS CLI, Infrastructure-as‚ÄëCode, and reproducible automation. Learned monthly operations: monitoring, backup/restore procedures and security best practices. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-deploybackend/5.3.1-architecture/","title":"Architecture Deep Dive","tags":[],"description":"","content":"Let\u0026rsquo;s dive into the BackendStack source code to understand how the system operates. Below are the detailed code snippets for each resource.\n1. Database (Amazon DynamoDB) We initialize 6 DynamoDB tables using PAY_PER_REQUEST (On-Demand) mode to optimize costs and scalability.\n// 1. Listings Table (Room rentals) const listingsTable = new dynamodb.Table(this, \u0026#34;ListingsTable\u0026#34;, { tableName: \u0026#34;BoardingHouseListings\u0026#34;, partitionKey: { name: \u0026#34;listingId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 2. User Profiles Table const userProfilesTable = new dynamodb.Table(this, \u0026#34;UserProfilesTable\u0026#34;, { tableName: \u0026#34;UserProfiles\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 3. OTP Table (Phone Verification) - Auto-delete after 5 mins (TTL) const otpTable = new dynamodb.Table(this, \u0026#34;OTPVerifications\u0026#34;, { tableName: \u0026#34;OTPVerifications\u0026#34;, partitionKey: { name: \u0026#34;phoneNumber\u0026#34;, type: dynamodb.AttributeType.STRING, }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, timeToLiveAttribute: \u0026#34;ttl\u0026#34;, // TTL configuration removalPolicy: RemovalPolicy.DESTROY, }); // 4. Favorites Table - Includes Sort Key for quick lookup by user const favoritesTable = new dynamodb.Table(this, \u0026#34;FavoritesTable\u0026#34;, { tableName: \u0026#34;UserFavorites\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, sortKey: { name: \u0026#34;listingId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 5. Support Requests Table const supportRequestsTable = new dynamodb.Table(this, \u0026#34;SupportRequestsTable\u0026#34;, { tableName: \u0026#34;SupportRequests\u0026#34;, partitionKey: { name: \u0026#34;requestId\u0026#34;, type: dynamodb.AttributeType.STRING, }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, removalPolicy: RemovalPolicy.DESTROY, }); // 6. Notifications Table - Includes TTL const notificationsTable = new dynamodb.Table(this, \u0026#34;NotificationsTable\u0026#34;, { tableName: \u0026#34;Notifications\u0026#34;, partitionKey: { name: \u0026#34;userId\u0026#34;, type: dynamodb.AttributeType.STRING }, sortKey: { name: \u0026#34;notificationId\u0026#34;, type: dynamodb.AttributeType.STRING }, billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, timeToLiveAttribute: \u0026#34;ttl\u0026#34;, removalPolicy: RemovalPolicy.DESTROY, }); 2. Storage (Amazon S3) An S3 Bucket is created to store room images, configured with security blocks against public access and auto-deletion when the stack is destroyed.\nconst imagesBucket = new s3.Bucket(this, \u0026#34;BoardingHouseImages\u0026#34;, { bucketName: `findnest-images-${cdk.Aws.ACCOUNT_ID}`, // Unique bucket name removalPolicy: RemovalPolicy.DESTROY, autoDeleteObjects: true, blockPublicAccess: s3.BlockPublicAccess.BLOCK_ALL, // Private by default }); 3. Authentication (Amazon Cognito) We configure a User Pool to manage users and an Identity Pool to grant the Frontend direct access to AWS resources.\n// Create User Pool const userPool = new cognito.UserPool(this, \u0026#34;UserPool\u0026#34;, { userPoolName: \u0026#34;FindNestUsers\u0026#34;, selfSignUpEnabled: false, // User created via API (Backend trigger) signInAliases: { phone: true, // User uses Phone Number username: true, // Admin uses Username }, autoVerify: { phone: true }, standardAttributes: { email: { required: false, mutable: true }, phoneNumber: { required: false, mutable: true }, }, passwordPolicy: { minLength: 8, requireLowercase: true, requireUppercase: true, requireDigits: true, requireSymbols: true, }, accountRecovery: cognito.AccountRecovery.PHONE_ONLY_WITHOUT_MFA, removalPolicy: RemovalPolicy.DESTROY, }); // Create Client App const userPoolClient = userPool.addClient(\u0026#34;UserPoolClient\u0026#34;, { authFlows: { userPassword: true, adminUserPassword: true, // Used for Backend auth flow custom: true, }, }); // User Groups const usersGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;UsersGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Users\u0026#34;, }); const landlordsGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;LandlordsGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Landlords\u0026#34;, }); const adminsGroup = new cognito.CfnUserPoolGroup(this, \u0026#34;AdminsGroup\u0026#34;, { userPoolId: userPool.userPoolId, groupName: \u0026#34;Admins\u0026#34;, }); // Identity Pool for Frontend const identityPool = new cognito.CfnIdentityPool(this, \u0026#34;IdentityPool\u0026#34;, { identityPoolName: \u0026#34;FindNestMapAccess\u0026#34;, allowUnauthenticatedIdentities: true, // Allow guests to view map cognitoIdentityProviders: [ { clientId: userPoolClient.userPoolClientId, providerName: userPool.userPoolProviderName, }, ], }); // IAM Role for unauthenticated users (map access only) const unauthenticatedRole = new iam.Role(this, \u0026#34;UnauthenticatedRole\u0026#34;, { assumedBy: new iam.FederatedPrincipal( \u0026#34;cognito-identity.amazonaws.com\u0026#34;, { StringEquals: { \u0026#34;cognito-identity.amazonaws.com:aud\u0026#34;: identityPool.ref, }, \u0026#34;ForAnyValue:StringLike\u0026#34;: { \u0026#34;cognito-identity.amazonaws.com:amr\u0026#34;: \u0026#34;unauthenticated\u0026#34;, }, }, \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34; ), inlinePolicies: { LocationServicePolicy: new iam.PolicyDocument({ statements: [ new iam.PolicyStatement({ effect: iam.Effect.ALLOW, actions: [ \u0026#34;geo:GetMap*\u0026#34;, \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:CalculateRouteMatrix\u0026#34;, ], resources: [map.attrArn, placeIndex.attrArn, routeCalculator.attrArn], }), ], }), }, }); // IAM Role for authenticated users (full access) const authenticatedRole = new iam.Role(this, \u0026#34;AuthenticatedRole\u0026#34;, { assumedBy: new iam.FederatedPrincipal( \u0026#34;cognito-identity.amazonaws.com\u0026#34;, { StringEquals: { \u0026#34;cognito-identity.amazonaws.com:aud\u0026#34;: identityPool.ref, }, \u0026#34;ForAnyValue:StringLike\u0026#34;: { \u0026#34;cognito-identity.amazonaws.com:amr\u0026#34;: \u0026#34;authenticated\u0026#34;, }, }, \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34; ), inlinePolicies: { LocationServicePolicy: new iam.PolicyDocument({ statements: [ new iam.PolicyStatement({ effect: iam.Effect.ALLOW, actions: [ \u0026#34;geo:GetMap*\u0026#34;, \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:CalculateRouteMatrix\u0026#34;, \u0026#34;geo:BatchGetDevicePosition\u0026#34;, \u0026#34;geo:GetDevicePosition\u0026#34;, ], resources: [map.attrArn, placeIndex.attrArn, routeCalculator.attrArn], }), ], }), }, }); // Attach roles to identity pool new cognito.CfnIdentityPoolRoleAttachment(this, \u0026#34;IdentityPoolRoleAttachment\u0026#34;, { identityPoolId: identityPool.ref, roles: { authenticated: authenticatedRole.roleArn, unauthenticated: unauthenticatedRole.roleArn, }, }); 4. Location Service (Maps \u0026amp; Geocoding) Initialize Location Service resources using Here data provider for better POI coverage in Vietnam.\nconst placeIndex = new location.CfnPlaceIndex(this, \u0026#34;PlaceIndex\u0026#34;, { indexName: `FindNestPlacesV3-${cdk.Aws.ACCOUNT_ID}`, dataSource: \u0026#34;Here\u0026#34;, // Better POI coverage for Asia (Vietnam) dataSourceConfiguration: { intendedUse: \u0026#34;Storage\u0026#34;, // Allows storing and querying POI data }, }); const map = new location.CfnMap(this, \u0026#34;Map\u0026#34;, { mapName: `FindNestMap-${cdk.Aws.ACCOUNT_ID}`, configuration: { style: \u0026#34;VectorEsriStreets\u0026#34; }, }); const routeCalculator = new location.CfnRouteCalculator( this, \u0026#34;RouteCalculator\u0026#34;, { calculatorName: `FindNestRoutesV3-${cdk.Aws.ACCOUNT_ID}`, dataSource: \u0026#34;Here\u0026#34;, } ); 5. Compute (AWS Lambda Monolith) Configure the Lambda Function containing the entire Backend logic, including full environment variable injection.\nconst apiLambda = new lambda.Function(this, \u0026#34;ApiLambda\u0026#34;, { functionName: \u0026#34;FindNestApi\u0026#34;, runtime: lambda.Runtime.NODEJS_20_X, handler: \u0026#34;index.handler\u0026#34;, code: lambda.Code.fromAsset(path.join(__dirname, \u0026#34;../../backend/src/lambda\u0026#34;)), timeout: cdk.Duration.seconds(30), logGroup: logGroup, environment: { // Environment variables connecting resources LISTINGS_TABLE_NAME: listingsTable.tableName, USER_PROFILES_TABLE_NAME: userProfilesTable.tableName, OTP_TABLE_NAME: otpTable.tableName, FAVORITES_TABLE_NAME: favoritesTable.tableName, SUPPORT_REQUESTS_TABLE_NAME: supportRequestsTable.tableName, NOTIFICATIONS_TABLE_NAME: notificationsTable.tableName, IMAGES_BUCKET_NAME: imagesBucket.bucketName, USER_POOL_ID: userPool.userPoolId, USER_POOL_CLIENT_ID: userPoolClient.userPoolClientId, PLACE_INDEX_NAME: placeIndex.indexName, MAP_NAME: map.mapName, ROUTE_CALCULATOR_NAME: routeCalculator.calculatorName, BEDROCK_MODEL_ID: \u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34;, REGION: cdk.Aws.REGION, }, }); 6. Permissions (Granular Access Control) We grant Granular Permissions to the Lambda Function. Here is the full list of granted permissions:\nA. Read/Write Access to Database and S3:\nlistingsTable.grantReadWriteData(apiLambda); userProfilesTable.grantReadWriteData(apiLambda); otpTable.grantReadWriteData(apiLambda); favoritesTable.grantReadWriteData(apiLambda); supportRequestsTable.grantReadWriteData(apiLambda); notificationsTable.grantReadWriteData(apiLambda); imagesBucket.grantReadWrite(apiLambda); B. User Management in Cognito (Full Admin Actions):\napiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [ \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminSetUserPassword\u0026#34;, \u0026#34;cognito-idp:AdminInitiateAuth\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:AdminAddUserToGroup\u0026#34;, \u0026#34;cognito-idp:AdminRemoveUserFromGroup\u0026#34;, \u0026#34;cognito-idp:AdminListGroupsForUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:AdminEnableUser\u0026#34;, \u0026#34;cognito-idp:AdminDisableUser\u0026#34;, \u0026#34;cognito-idp:AdminDeleteUser\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-idp:GlobalSignOut\u0026#34;, ], resources: [userPool.userPoolArn], }) ); C. Integrations with other services (SNS, Bedrock, Location):\n// Send SMS (OTP) apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [\u0026#34;sns:Publish\u0026#34;], resources: [\u0026#34;*\u0026#34;], }) ); // Invoke AI (Claude 3) apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [\u0026#34;bedrock:InvokeModel\u0026#34;], resources: [\u0026#34;arn:aws:bedrock:*::foundation-model/anthropic.claude-3-*\u0026#34;], }) ); // Access Location Service apiLambda.addToRolePolicy( new iam.PolicyStatement({ actions: [ \u0026#34;geo:SearchPlaceIndexForText\u0026#34;, \u0026#34;geo:GetPlace\u0026#34;, \u0026#34;geo:CalculateRoute\u0026#34;, \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, ], resources: [placeIndex.attrArn, routeCalculator.attrArn], }) ); 7. API Gateway (REST API) Create a public Endpoint for clients to call the Lambda function.\nconst api = new apigateway.LambdaRestApi(this, \u0026#34;BoardingHouseApi\u0026#34;, { handler: apiLambda, proxy: true, deployOptions: { stageName: \u0026#34;prod\u0026#34;, throttlingBurstLimit: 100, throttlingRateLimit: 50, }, defaultCorsPreflightOptions: { allowOrigins: apigateway.Cors.ALL_ORIGINS, allowMethods: apigateway.Cors.ALL_METHODS, allowHeaders: [\u0026#34;Content-Type\u0026#34;, \u0026#34;Authorization\u0026#34;], }, restApiName: \u0026#34;FindNestAPI\u0026#34;, }); 8. Monitoring \u0026amp; Observability (CloudWatch) We implement comprehensive monitoring with CloudWatch Dashboard, Alarms, and SNS Notifications.\nA. SNS Alert Topic:\nconst alertTopic = new sns.Topic(this, \u0026#34;AlertTopic\u0026#34;, { topicName: \u0026#34;BoardingHouseAlerts\u0026#34;, displayName: \u0026#34;Smart Boarding House Alerts\u0026#34;, }); alertTopic.addSubscription( new snsSubscriptions.EmailSubscription(\u0026#34;admin@smartboardinghouse.com\u0026#34;) ); B. CloudWatch Alarms:\n// Lambda Error Alarm const lambdaErrorAlarm = new cloudwatch.Alarm(this, \u0026#34;LambdaErrorAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-Lambda-Errors\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/Lambda\u0026#34;, metricName: \u0026#34;Errors\u0026#34;, dimensionsMap: { FunctionName: lambdaFunctionName }, statistic: \u0026#34;Sum\u0026#34;, }), threshold: 5, evaluationPeriods: 2, }); // Lambda Duration Alarm const lambdaDurationAlarm = new cloudwatch.Alarm(this, \u0026#34;LambdaDurationAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-Lambda-Duration\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/Lambda\u0026#34;, metricName: \u0026#34;Duration\u0026#34;, dimensionsMap: { FunctionName: lambdaFunctionName }, statistic: \u0026#34;Average\u0026#34;, }), threshold: 25000, // 25 seconds evaluationPeriods: 3, }); // API Gateway 4xx/5xx Alarms const apiGateway4xxAlarm = new cloudwatch.Alarm(this, \u0026#34;ApiGateway4xxAlarm\u0026#34;, { alarmName: \u0026#34;BoardingHouse-API-4xx-Errors\u0026#34;, metric: new cloudwatch.Metric({ namespace: \u0026#34;AWS/ApiGateway\u0026#34;, metricName: \u0026#34;4XXError\u0026#34;, dimensionsMap: { ApiName: apiGatewayName }, statistic: \u0026#34;Sum\u0026#34;, }), threshold: 10, evaluationPeriods: 2, }); C. CloudWatch Dashboard (8 Rows):\nconst dashboard = new cloudwatch.Dashboard(this, \u0026#34;BoardingHouseDashboard\u0026#34;, { dashboardName: \u0026#34;SmartBoardingHouse-Monitoring\u0026#34;, defaultInterval: cdk.Duration.hours(24), }); // Row 1: Lambda Overview (Invocations, Errors, Throttles) dashboard.addWidgets( new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Invocations\u0026#34;, left: [lambdaInvocationsMetric], width: 8, height: 6, }), new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Errors\u0026#34;, left: [lambdaErrorsMetric], width: 8, height: 6, }), new cloudwatch.GraphWidget({ title: \u0026#34;Lambda Throttles\u0026#34;, left: [lambdaThrottlesMetric], width: 8, height: 6, }) ); // Row 2: Lambda Performance (Duration, Concurrent Executions) // Row 3: API Gateway (Requests, Latency) // Row 4: API Gateway Errors (4XX, 5XX) // Row 5: DynamoDB Metrics (Read/Write Capacity, Errors) // Row 6: System Health Summary (Error Rates, Response Time, Total Requests) // Row 7: Bedrock AI Token Usage \u0026amp; Invocations // Row 8: Bedrock Model Summary (Total Tokens, Invocations, Latency) Dashboard Features:\nüìä Lambda Metrics: Invocations, Errors, Throttles, Duration, Concurrency üåê API Gateway Metrics: Requests, Latency (Avg \u0026amp; P99), 4XX/5XX Errors üíæ DynamoDB Metrics: Read/Write Capacity, User Errors per table ü§ñ Bedrock AI Metrics: Token Usage (Input/Output), Invocations, Latency üìà System Health Summary: Error Rates, Response Time, Total Requests (24h) üö® Automatic Alerts: Email notifications via SNS when thresholds are breached "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/5.6.1-destroy-stack/","title":"Destroy the Stack","tags":[],"description":"","content":"The beauty of Infrastructure as Code (IaC) with AWS CDK is that you can dismantle the entire system with a single command.\nCDK knows exactly what it created and will delete resources in the correct dependency order.\nNavigate to CDK Directory Make sure you are in the cdk directory:\ncd cdk Run Destroy Command Execute the following command:\ncdk destroy Confirmation Prompt The CLI will ask for confirmation:\nAre you sure you want to delete: BackendStack (y/n)? Type y and press Enter.\nDeletion Process The deletion process generally takes 3-5 minutes.\nYou will see output similar to:\nBackendStack: destroying... ‚úÖ BackendStack: destroyed Stack ARN: arn:aws:cloudformation:us-east-1:123456789012:stack/BackendStack/... What Gets Deleted CDK will automatically delete all resources in the correct order:\nAPI Gateway - REST API and deployment Lambda Function - Including associated IAM roles DynamoDB Tables - All 7 tables Cognito User Pool - Users and groups Cognito Identity Pool - Identity mappings S3 Bucket - Including all objects (due to autoDeleteObjects: true) Location Service - Maps, Place Index, Route Calculator CloudWatch Log Groups - Lambda execution logs IAM Roles and Policies - All service permissions CloudFormation Stack - The stack itself The RemovalPolicy.DESTROY configuration in our CDK code ensures that data-containing resources (like DynamoDB tables and S3 buckets) are also deleted.\nMonitor Deletion Progress You can monitor the deletion progress in:\nTerminal: Watch the CLI output AWS Console: Go to CloudFormation ‚Üí Stacks ‚Üí BackendStack ‚Üí Events tab Troubleshooting Deletion Issues If the deletion fails:\nIssue: S3 Bucket Not Empty\n# Manually empty the bucket aws s3 rm s3://findnest-images-YOUR-ACCOUNT-ID --recursive # Then retry destroy cdk destroy Issue: DynamoDB Table Has Deletion Protection\nIf you modified the tables manually in console and enabled deletion protection, you need to disable it first:\nGo to DynamoDB Console Select the table Go to Additional settings tab Turn off Deletion protection Retry cdk destroy Issue: Lambda Execution Role In Use\nWait a few minutes for Lambda executions to complete, then retry.\nOnce you run cdk destroy and confirm, the process cannot be undone. All data will be permanently deleted.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"AWS Cloud Mastery Series #1 ‚Äî AI/ML \u0026amp; Generative AI on AWS\nTime: Saturday, 15 November 2025 | 08:30 - 12:00 (GMT+7)\nLocation: Bitexco Financial Tower, District 1, Ho Chi Minh City\nEvent Overview This session is part of the AWS Cloud Mastery Series, focused on AI/ML and Generative AI on AWS. The event aimed to provide a practical overview of AWS AI/ML services (including Amazon SageMaker and Amazon Bedrock), introduce prompt engineering and Retrieval-Augmented Generation (RAG) patterns, and demonstrate how to integrate foundation models into real applications.\nOrganizers \u0026amp; Speakers Host: Kha Van (Luma) Speakers / Guests: Quan Nguyen, Ikaris Dinh (ƒêinh Vi·ªát Hi·∫øu), and other experts from the AWS community Highlights \u0026amp; Value Guidance on selecting appropriate foundation models and understanding trade-offs Hands-on prompt engineering patterns and building RAG pipelines to improve response quality Live demo: building a generative chatbot using Amazon Bedrock Integrating SageMaker into MLOps pipelines for model lifecycle management Personal Takeaways \u0026amp; Practical Applications Ideas for applying RAG to internal knowledge bases and customer support systems Approaches to using Bedrock Agents for orchestrating complex automation workflows Techniques for content safety and reducing risks when deploying generative systems Networking opportunities provided access to practical resources and community support Key Takeaways (quick) Amazon SageMaker enables an end-to-end ML workflow from data to deployment. Amazon Bedrock provides access to foundation models and makes it easier to integrate multiple models into a production flow. RAG combined with a retrieval system is an effective pattern to improve accuracy and relevance for domain-specific generative AI. "},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Le Dang Khoa\nPhone Number: 0816.324.990\nEmail: nkhoa5861@gmail.com\nUniversity: FPT Unversity\nMajor: Information Technology\nClass: IA1801\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 06/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Serverless Architecture Serverless architecture allows you to build and run applications and services without thinking about servers. It eliminates infrastructure management tasks such as server provisioning, patching, operating system maintenance, and capacity provisioning. The architecture relies on AWS Lambda for compute, Amazon API Gateway for API management, and Amazon DynamoDB for data storage. Compute resources running in Lambda are triggered by events from API Gateway and scale automatically.\nWorkshop overview In this workshop, you will deploy the Findnest Backend using AWS CDK. \u0026ldquo;Backend Logic\u0026rdquo; is handled by AWS Lambda running Node.js/Express code. It serves as the \u0026ldquo;brain\u0026rdquo; of the application, processing business logic only when triggered. \u0026ldquo;Data \u0026amp; Storage\u0026rdquo; utilizes Amazon DynamoDB for high-performance NoSQL data storage (Listings, Users) and Amazon S3 for storing user-uploaded images securely. Amazon API Gateway acts as the secure entry point for all client requests.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-systemvalidation/5.5.1-retrieve-config/","title":"Retrieve Configuration","tags":[],"description":"","content":"Ensure you have the ApiUrl from the outputs of Section 5.3 (Deploy Backend).\nLocate the API URL After running cdk deploy, you should have received outputs similar to:\n‚úÖ BackendStack Outputs: BackendStack.ApiUrl = https://xyz123.execute-api.us-east-1.amazonaws.com/prod/ BackendStack.UserPoolId = us-east-1_AbCdEfGh ... API URL Format The API URL follows this format:\nhttps://\u0026lt;api-id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/prod/ Example:\nhttps://xyz123.execute-api.us-east-1.amazonaws.com/prod/ The /prod/ at the end indicates the deployment stage. This is where your API Gateway routes all incoming requests.\nSave the API URL Copy and save this URL. You will use it as the base URL for all API requests in the following test cases.\nIf you lost the outputs, you can retrieve them by:\nRunning cdk deploy again (it won\u0026rsquo;t redeploy if nothing changed) Checking the CloudFormation stack outputs in AWS Console Going to API Gateway Console and finding your API\u0026rsquo;s invoke URL Verify API Gateway You can quickly verify that API Gateway is accessible by opening the URL in a browser:\nhttps://xyz123.execute-api.us-east-1.amazonaws.com/prod/ You should see a response indicating the API is running (might be a 404 or a welcome message, depending on your root route configuration).\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-seedingdata/5.4.1-setup-script/","title":"Setup the Script","tags":[],"description":"","content":"Step 1: Navigate to the directory Navigate to the scripts directory where the seeding script is located:\ncd backend/scripts Step 2: Install dependencies The script uses the AWS SDK v3. Install the required packages:\nnpm install This will install all dependencies defined in the package.json file, including:\n@aws-sdk/client-cognito-identity-provider - For Cognito operations @aws-sdk/client-dynamodb - For DynamoDB operations dotenv - For environment variable management Other required dependencies Make sure you\u0026rsquo;re in the backend/scripts directory before running npm install.\nWait for the installation to complete successfully before proceeding to the next step.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Proposed Project: Serverless Photo Analytics \u0026amp; Moderation Objectives Build a serverless photo album application that enables users to sign in, upload photos, automatically tag (labels) and moderate content, search by labels, distribute content globally via a CDN, and provide an analytics dashboard with usage insights and cost alerts. The project is optimized for a team of five roles: 2 Front-end, 1 Back-end, 1 Security, and 1 Research.\nKey Value Propositions (Highlights) Cloud-native, serverless architecture: highly scalable and cost-efficient. Aligned with the five Well-Architected pillars: Security, Reliability, Performance, Cost, and Operational Excellence. Full CI/CD, IaC, and observability for a professional demo. Target Architecture (AWS) Frontend: S3 static website + CloudFront (HTTPS, caching, edge). Optional A/B testing using CloudFront Functions. AuthN/AuthZ: Amazon Cognito (User Pool, Hosted UI, Group/Role mapping for Basic vs Admin users). API: Amazon API Gateway (REST) or AWS AppSync (GraphQL). AppSync is recommended for realtime subscriptions for moderation logs. Compute: AWS Lambda (Node.js/Python) for business logic. Storage: Original photos: Amazon S3 (SSE-KMS encryption, block public access, presigned URL uploads). Metadata: Amazon DynamoDB (Photos, Labels, Moderation queue). Use secondary indexes to query by user, label, or status. Event processing: S3 Event ‚Üí SQS (decouple) ‚Üí Lambda processing (Rekognition DetectLabels + ModerationLabels). Step Functions for orchestration: write metadata ‚Üí detect labels ‚Üí update status ‚Üí notify. Advanced search (optional): Amazon OpenSearch Serverless for rich search across multiple fields/labels. Observability: CloudWatch Logs, Metrics, Alarms, X-Ray tracing; AppConfig/Parameter Store for runtime configuration. Analytics / BI: Event logs (views, searches, uploads) forwarded to Kinesis Firehose ‚Üí S3 (data lake). Use Glue Crawler + Athena for queries and QuickSight for dashboards. Security: AWS WAF attached to CloudFront; Shield Standard; IAM least-privilege; strict S3 bucket policies; Secrets Manager/KMS. CI/CD \u0026amp; IaC: IaC: AWS CDK (TypeScript) or Terraform (CDK recommended for AWS consistency). Monorepo for FE + BE + IaC. CI/CD: CodePipeline (GitHub/CodeCommit ‚Üí CodeBuild ‚Üí CDK deploy) or GitHub Actions + CDK. Main Flow User signs in with Cognito ‚Üí Frontend receives ID token. Frontend requests a presigned URL ‚Üí uploads the photo to S3. S3 emits an event ‚Üí SQS ‚Üí Lambda handles: Rekognition for labels + moderation, then writes metadata to DynamoDB. Suspected images ‚Üí set state to \u0026ldquo;PENDING_REVIEW\u0026rdquo;. Admin UI receives realtime events (AppSync subscription) for manual review. The frontend Gallery queries only APPROVED images; supports filtering by label; images are served through CloudFront (signed cookies/URLs if needed). Events and usage data are streamed to S3 ‚Üí Athena/QuickSight for dashboards. Roles \u0026amp; Deliverables FE 1 (Gallery Web): SPA: Sign-in, upload (presigned URL), gallery, label-based filtering, infinite scroll. Optimizations: lazy-loading, responsive design, CloudFront cache control. Acceptance: Lighthouse Performance ‚â• 85; end-to-end functionality. FE 2 (Admin Moderation): Moderation UI: view labels/moderation scores, approve/reject, tag editor. Realtime feed (AppSync subscriptions) and audit logs. Acceptance: user actions respond within 2s, pagination, audit trail. Back-end: AppSync schema/resolvers or API Gateway routes; Lambda functions; Step Functions; SQS; DynamoDB schema (with GSIs); presigned URL service. Observability: structured logs, X-Ray, CloudWatch dashboards; error-budget SLO. Acceptance: 90% unit test coverage for backend; p95 API latency \u0026lt; 300ms (with cache hits); reproducible IaC deployments. Security: IAM least-privilege; S3/KMS; WAF rules (rate-based, IP allow/deny, basic bot control); Cognito password policies + email OTP; Config rules + Security Hub findings triage; budget alerts. Basic pen-test (OWASP ASVS check) and a security report aligned to the Well-Architected Security pillar. Acceptance: no High/Critical findings from scans; WAF blocks demo payloads; compliance report. Research: Design A/B experiments (e.g., floating upload button vs fixed placement) at the edge; collect metrics; analyze via Athena/QuickSight; recommend cost/perf improvements. Benchmark Rekognition thresholds and choose appropriate cut-offs; evaluate OpenSearch Serverless vs DynamoDB GSI for search. Acceptance: experimental report with metrics and actionable conclusions; clear PRD updates. Example Grading Rubric (Suggested) Architecture \u0026amp; IaC (25%): clear diagrams, repeatable CDK/Terraform, multi-environment deployments. Security (20%): IAM least-privilege, KMS, WAF, private-by-default, auditability. Product Quality (20%): smooth UX, robust error handling, basic accessibility. Operations \u0026amp; Observability (15%): logging/metrics/traces, alarms, runbooks. Performance \u0026amp; Cost (10%): cache hit rate, p95 latency, budget alerts. Documentation \u0026amp; Presentation (10%): README, diagrams, demo script, A/B results. 6‚Äì8 Minute Demo Script Architecture diagram (1\u0026rsquo;). Sign-in and upload demo (1\u0026rsquo;). Automated labeling + moderation workflow (1.5\u0026rsquo;). Admin realtime moderation (1\u0026rsquo;). Search by label + CDN distribution (1\u0026rsquo;). Athena/QuickSight dashboard + budget alerts (1\u0026rsquo;). WAF rule demonstrably blocking a sample payload (30s). 3‚Äì4 Week Roadmap Week 1: IaC scaffolding (VPC optional), Cognito, S3, CloudFront, AppSync/API, DynamoDB; frontend skeleton; CI/CD pipeline. Week 2: Upload flow + presigned URL; S3‚ÜíSQS‚ÜíLambda‚ÜíRekognition‚ÜíDynamoDB; basic gallery UI; logging and dashboards. Week 3: Admin moderation + subscriptions; WAF \u0026amp; security hardening; event data lake (Firehose‚ÜíS3‚ÜíGlue‚ÜíAthena); budget alerts. Week 4 (buffer): QuickSight dashboards, performance tuning, A/B experiments, final report \u0026amp; rehearsal. Stretch Goals (Extra Credit) Textract OCR for text extraction from images to support text search. Signed URL / Signed Cookies for private images; PWA offline support. OpenSearch Serverless for full-text/aggregations; label suggestions using Bedrock Titan (if access is available). Multi-tenant support (namespace per Cognito sub / tenant id). Estimated Cost (Low) Friendly to free-tier: S3, DynamoDB (on-demand low usage), Lambda, CloudFront for small demos; Rekognition billed per image (manageable for demo datasets). Set a Budget alert at $5‚Äì10. Accompanying Documentation to Create in the Repo Architecture diagrams (draw.io/png), README, runbook (common incidents), example policy-as-code (cfn-guard/OPA), and a security checklist. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 11/09/2025 11/09/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database\n12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 13/09/2025 13/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + SG, Keypair, IP - SSH connection methods to EC2 - Learn about Elastic IP 14/09/2025 14/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Team \u0026amp; Orientation:\nConnected with First Cloud Journey (FCJ) members and reviewed internship rules and expectations (Day 2). Account \u0026amp; Access:\nCreated an AWS Free Tier account and verified access via the Console. Installed and configured AWS CLI locally via aws configure. Created an IAM user with programmatic access and applied minimal permissions for learning tasks. Compute (EC2) \u0026amp; Block Storage (EBS):\nLaunched an EC2 instance (t2.micro), created a Key Pair, configured Security Group rules, and connected via SSH. Created and attached an EBS volume to the instance, mounted it and verified read/write operations. Knowledge \u0026amp; Skills:\nGained foundational knowledge of AWS service categories relevant to practical labs: Compute, (Block) Storage (EBS), Networking, Database, and Security. Learned EC2 key concepts: Instance Types, AMIs, EBS, Security Groups, Key Pairs and Elastic IP. Practiced using both AWS Console and AWS CLI for provisioning and basic resource management. Documentation \u0026amp; Next Steps:\nDocumented CLI commands and step-by-step procedures in the worklog for reproducibility and future reference. Next steps: explore IAM roles/policies, VPC fundamentals, and S3/object lifecycle in Week 2 (S3 operations will be covered later). "},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"FindNest AWS Serverless AI Platform for Smart Accommodation Search 1. Executive Summary The FindNest platform leverages AI understanding and AWS Serverless architecture to transform accommodation search into a contextual, intelligent experience. By combining Amazon Bedrock for natural language processing and Amazon Location Service for spatial analysis, it enables users to find rooms using natural queries like \u0026ldquo;affordable room near Thu Duc with gym and safe area.\u0026rdquo;\nThe system (Frontend: React hosted on Amplify, Backend: AWS Lambda + API Gateway) stores data in DynamoDB, handles authentication via Cognito, and enriches listings with contextual insights such as food density, nearby amenities, and safety index. Notifications and OTP authentication are handled via Amazon SNS. The entire platform operates under AWS Free Tier with an estimated cost of ~$0.5/month.\n2. Problem Statement What\u0026rsquo;s the Problem? Current platforms only support simple filters such as price or area and lack the ability to understand nuanced user intent. Users must manually review hundreds of listings without AI assistance or location intelligence. There‚Äôs no mechanism to understand complex preferences like ‚Äúsafe neighborhood with good food options‚Äù or ‚Äúeasy commute to District 1.‚Äù\nThe Solution An AI-enhanced, serverless platform that interprets user intent through natural language, automatically enriches listings with contextual data (restaurants, safety, routes), and recommends relevant results using Amazon Bedrock and Amazon Location Service. The backend filters listings stored in DynamoDB and ranks results using AI scoring.\nBenefits and Return on Investment AI Semantic Search: Bedrock interprets user intent beyond filters. Contextual Recommendations: Listings enriched by Location Service provide real-world relevance. Serverless Scalability: Fully managed services scale automatically with zero maintenance. Cost Efficiency: All components run within AWS Free Tier for MVP phase (~$0.5/month). 3. Solution Architecture The platform utilizes a modular AWS Serverless design with AI enrichment and dynamic contextual data.\nComponent Service / Technology Frontend Hosting AWS Amplify (React SPA) API Backend AWS Lambda + API Gateway Database DynamoDB File Storage S3 User Management Cognito Notifications Amazon SNS Map \u0026amp; Location Amazon Location Service Recommendation Engine Amazon Bedrock + Lambda Logic AWS Services Used AWS Lambda: Executes backend logic including AI processing and search queries. Amazon API Gateway: Provides RESTful endpoints for client requests. Amazon DynamoDB: Stores user profiles, listings, and enriched context data. Amazon S3: Stores room images and frontend static files. AWS Amplify: Hosts and manages frontend deployment. Amazon Cognito: Manages authentication and authorization flows. Amazon SNS: Sends OTP codes and user notifications. Amazon Location Service: Fetches surrounding POIs, routes, and safety context. Amazon Bedrock: Interprets natural language search and performs semantic ranking. Component Design Frontend Application: React SPA hosted on Amplify for responsive, dynamic user experience. API Layer: Express app deployed on Lambda via API Gateway handling AI search, listings, and enrichment. Database: DynamoDB tables for listings, users, and search history. Storage: S3 bucket stores images; public read via signed URLs. Recommendation Engine: Bedrock interprets user queries and ranks listings. Map Integration: Amazon Location Service provides contextual location and POI visualization. Notification System: SNS delivers OTPs and alerts for new listings. User Management: Cognito handles registration, login, and secure tokens. 4. Technical Implementation Recommendation Engine Approach\nMVP Phase: Bedrock interprets user query ‚Üí DynamoDB filters + Location Service enrichment. Expansion Phase: Continuous enrichment jobs to compute contextual indexes (food_density, safety_score, comfort_index). Advanced Phase: Adaptive learning ‚Äî store user feedback to refine Bedrock prompt responses. Technical Requirements\nFrontend: React + Amplify UI with AI-integrated search bar and location maps. Backend: Node.js Lambda app using AWS SDK for Bedrock, DynamoDB, and Location. Database: DynamoDB tables for users, listings, and search history. Storage: S3 buckets for file storage. Authentication: Cognito + SNS OTP login flow. 5. Timeline \u0026amp; Milestones Project Timeline\nWeek 1-2: Design AWS architecture, configure Amplify, and deploy base API. Week 3-4: Implement Bedrock semantic search and Location enrichment logic. Week 5: Integrate frontend and refine AI-driven search flow. Week 6: Finalize testing and deployment. Post-Launch: Collect user data for AI improvement and feedback loops. 6. Budget Estimation Infrastructure Costs Component Service Estimated Cost Lambda + API Gateway Backend $0.22/month DynamoDB Database $0.10/month S3 Storage $0.20/month Cognito + SNS Authentication + OTP $0.13/month Bedrock AI Processing $7.5/month Location Service Map \u0026amp; Geospatial Data $3.30 Total ~$24.32/month Note: All services operate under Free Tier usage limits during MVP phase with minimal operational cost.\n7. Risk Assessment Risk Matrix Bedrock Misinterpretation: Medium impact, medium probability. Lambda Cost Spike (Scaling): Low impact, medium probability. Incomplete Contextual Data: Medium impact, low probability. Mitigation Strategies Prompt Engineering: Optimize Bedrock input and fallback to simple filters. Caching: Cache Location and AI results for frequent queries. Contingency Plans Bedrock Limitations: Fallback to DynamoDB-only filter logic. Timeout Issues: Split enrichment jobs into smaller Lambda batches. High API Load: Scale API Gateway with usage throttling. 8. Expected Outcomes Technical Improvements AI-powered natural language search via Bedrock. Automatic contextual enrichment using Location Service. Scalable, low-cost infrastructure powered by AWS Serverless stack. Long-term Value Continuous Learning: Improve Bedrock prompts with user feedback. Smart Context Awareness: Build dynamic profiles for regions and user habits. Scalable Foundation: Ready for integration with Amazon Personalize or Bedrock fine-tuning. Cost Efficiency: Fully serverless with minimal maintenance and no fixed servers. Download the Proposal\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Project Proposal: Intelligent Web Security Solution on AWS A feasible project idea is to build an automated web application security system on AWS, combining AWS security services with AI/ML tools. Specifically, we deploy a sample website (e.g., using Amazon S3/EC2 + CloudFront) and protect it with AWS WAF (Web Application Firewall) and AWS Shield Standard to prevent network attacks (e.g., network-layer DDoS)[1][2]. AWS WAF allows defining HTTP filtering rules, including pre-built OWASP Top 10 managed rule groups[2]. Additionally, we can use WAF\u0026rsquo;s rate-based rules to automatically block IPs generating abnormal requests (simulating HTTP flood attacks)[3]. The reference architecture diagram below illustrates a security solution integrating AWS WAF with other components (WAF serves as the central checkpoint for all requests)[4][1].\nFigure: Sample architecture diagram of web application security solution on AWS, with AWS WAF (web ACL) as the central component, integrating AWS Shield (DDoS) and automated log processing components for attack detection (source: AWS Solutions).\nKey AWS components in the project may include:\nAWS WAF and Shield Standard: WAF protects web apps from common vulnerabilities (OWASP Top 10 level) using managed rules[2]. Shield Standard provides free basic network DDoS protection for all AWS accounts[1], helping protect websites from basic DDoS attacks. AWS CloudFront or ALB: Used as CDN/load balancer (with free Shield integration) and logs access. CloudFront free-tier provides monthly free data transfer, helping optimize initial costs. AWS CloudWatch Metrics \u0026amp; Logs: Collects metrics (requests/sec, latency\u0026hellip;) and logs from CloudFront/ALB. CloudWatch service has built-in ML-based anomaly detection to identify anomalies in metrics (e.g., traffic spikes)[5]. AWS CloudWatch Logs can also detect anomalous patterns in application logs[6]. AWS Lambda + Athena: Lambda functions automatically process logs and perform analysis. For example, Lambda can analyze WAF logs from S3 or CloudWatch to detect scanning behavior (scanner/probe) and automatically update WAF\u0026rsquo;s IP blocklist[7]. Athena can also be used to query large logs. AWS SageMaker (AI/ML): Build ML models to detect anomalies in traffic. For example, use the unsupervised Random Cut Forest (RCF) algorithm on SageMaker to assign anomaly scores to each traffic data point (requests, bytes, etc.)[8]. SageMaker offers a 2-month free-tier (250 CPU hours/month) for model training[9]. AWS SNS (Simple Notification Service): Sends automatic alerts to the development team. SNS supports email, SMS, or Slack/Teams integration via webhooks. AWS Free Tier allows 1 million free SNS notifications per month[10], which is more than sufficient for sending incident alerts. With this design, the system workflow could be as follows: access data (traffic) is recorded by CloudFront/ALB and sent to CloudWatch Logs. CloudWatch Metrics monitors indicators (traffic volume, errors, etc.) and uses anomaly detection (ML) to detect spikes[5]. For application log analysis, we can create a Log Anomaly Detector in CloudWatch Logs, as it uses ML to learn normal patterns and automatically flag anomalous events (e.g., multiple 5xx errors, unfamiliar IPs\u0026hellip;)[6]. Simultaneously, we can create a Lambda function that periodically analyzes logs (or uses Athena) to identify attack patterns (such as URL scanning, SQL injection) and dynamically update WAF rules. When suspicious events occur, the system automatically uses Lambda to add suspicious IPs to WAF\u0026rsquo;s blocklist and sends notifications via SNS to the dev team.\nAI/ML Programming and Anomaly Detection The project needs to use AI/ML capabilities to detect unknown threats in advance. Besides using CloudWatch\u0026rsquo;s integrated ML, the team can build additional custom ML models. For example, collect access logs in S3, extract features like requests/minute, error count, packet size, and train SageMaker\u0026rsquo;s RCF model to classify anomalous data points[8]. The RCF model assigns an anomaly score to each time point, allowing alerts when the score exceeds a threshold. AWS provides free 2-month SageMaker access (250 CPU hours/month) for new accounts[9], so we can leverage this for rapid model development. After training, inference can be deployed via API to evaluate new logs.\nAdditionally, AWS GuardDuty is an option for security monitoring: it continuously monitors VPC Flow Logs, DNS Logs, and CloudTrail to detect malicious activity (abnormal API calls, malware) using ML[11]. However, GuardDuty typically requires payment after a 30-day trial. If we want to leverage the short trial period, the team can use GuardDuty only during the testing phase and focus on building custom ML through SageMaker or CloudWatch to avoid significant costs.\nUser Interaction \u0026amp; Monitoring Interface To give the frontend team work, we can design a simple web dashboard (React/Angular) displaying security alerts and metrics. For example, the interface could show charts of normal vs. abnormal traffic, list of blocked IPs, summarized attack logs, and sent alerts. AWS Amplify or S3+CloudFront can be used to host the frontend at very low cost (Amplify free-tier ~15GB). When the system detects incidents, SNS can send email/SMS directly to developers or send to Slack/Teams channels via webhook. This allows developers to quickly learn about anomalies and trace incidents.\nWork Division \u0026amp; Timeline With a 5-person team (2 Front-End, 1 Back-End, 2 Research) and a 2-month project duration, work can be divided as follows:\n- Phase 1 (weeks 1-2): Set up basic architecture. Back-End/Research (BE) builds sample website (or uses existing site) on AWS, installs CloudFront/ALB + connects WAF/Shield. Research (FE) studies OWASP rules, integrates WAF managed rules.\n- Phase 2 (weeks 2-3): Deploy log collection. BE and Lambda configure to push access logs (WAF/ALB/CloudFront) to CloudWatch Logs and S3. FE Research designs basic UI to display logs/test data.\n- Phase 3 (weeks 3-4): Build anomaly detection. Use CloudWatch Anomaly Detection to monitor metrics, while training ML model (SageMaker RCF) with historical data, integrating it in Lambda to detect unusual patterns.\n- Phase 4 (weeks 5-6): Implement automated response. Code Lambda to automatically update WAF rules (block IP) and call SNS to send alerts when attack signs are detected (via ML or Alarms). Design alert reporting interface. Front-End completes dashboard.\n- Phase 5 (weeks 7-8): Testing \u0026amp; optimization. Create attack scenarios (SQLi, simulated DDoS) for testing. Fine-tune WAF rules, alert thresholds, and ensure costs remain within budget.\nCost Optimization AWS provides many security services in Free Tier or free to reduce costs for proof-of-concept projects. For example: AWS Shield Standard provides free basic DDoS protection for all CloudFront/ELB[1]. AWS WAF has per-request fees, but initially with low traffic, costs are minimal. CloudWatch has a free-tier of 5GB log ingestion/month, 3 dashboards, 10 free metrics[12]. Amazon SNS offers 1 million free notifications/month[10]. Amazon SageMaker has a 2-month free-tier (250 CPU hours/month)[9] for ML use. If integrating AWS Lambda, Lambda offers 1M free invocations and 400k GB-seconds of free compute. Storage (S3) and CDN (CloudFront) services also have free tiers sufficient for small demos.\nOnly when data volume or traffic exceeds free-tier will costs be incurred, and we can safely limit to under ~$50-100. To optimize, we can proactively delete/suspend experimental services when not in use. In summary, this project is entirely feasible within the free tier or at very low cost, while meeting the requirement to apply AI/ML for monitoring and automatically responding to web threats.\nReferences: AWS Security Free Tier (Shield Standard free)[1]; AWS WAF automations (OWASP rules, rate-based rules)[2][3]; AWS CloudWatch Anomaly Detection (ML anomaly detection)[5]; CloudWatch Logs Anomaly Detection (unusual pattern detection)[6]; SageMaker Free Tier (250 CPU hours for training)[9] and Random Cut Forest algorithm[8]; SNS Free Tier (1M notifications)[10].\n[1] [11] [12] Free Cloud Security - AWS\nhttps://aws.amazon.com/free/security/\n[2] [3] [4] [7] Architecture overview - Security Automations for AWS WAF\nhttps://docs.aws.amazon.com/solutions/latest/security-automations-for-aws-waf/architecture-overview.html\n[5] Using CloudWatch anomaly detection - Amazon CloudWatch\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Anomaly_Detection.html\n[6] Log anomaly detection - Amazon CloudWatch Logs\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/LogsAnomalyDetection.html\n[8] Random Cut Forest (RCF) Algorithm - Amazon SageMaker AI\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html\n[9] Machine Learning Service - Amazon Sagemaker AI - AWS\nhttps://aws.amazon.com/sagemaker/ai/\n[10] Push Notification Service - Amazon Simple Notification Service (SNS) - AWS\nhttps://aws.amazon.com/sns/\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-seedingdata/5.4.2-configure-environment/","title":"Configure Environment","tags":[],"description":"","content":"The script (seed-admin.js) looks for specific environment variables to connect to your AWS resources. You need to create a .env file with values from the CDK Deployment Output.\nCreate the .env file Create a file named .env in the backend/scripts folder with the following content:\n# .env file content REGION=us-east-1 USER_POOL_ID=\u0026lt;Paste_Value_From_CDK_Output\u0026gt; USER_PROFILES_TABLE_NAME=UserProfiles Environment Variables Explained Variable Description Where to find REGION AWS region where resources are deployed us-east-1 (or ap-southeast-1 if you changed it) USER_POOL_ID Cognito User Pool identifier Copy the value of BackendStack.UserPoolId from the terminal after cdk deploy USER_PROFILES_TABLE_NAME DynamoDB table name for user profiles In your CDK code, we set the table name to UserProfiles. If you changed it, check the DynamoDB Console for the exact name Example Configuration # Example .env file REGION=us-east-1 USER_POOL_ID=us-east-1_AbCdEfGh USER_PROFILES_TABLE_NAME=UserProfiles Make sure to replace \u0026lt;Paste_Value_From_CDK_Output\u0026gt; with the actual value from your CDK deployment outputs!\nYou can find the CDK outputs by scrolling up in your terminal or by running cdk deploy again (it won\u0026rsquo;t redeploy if nothing changed).\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"GenAI-powered App-DB Modernization workshop Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-deploybackend/5.3.2-install-dependencies/","title":"Install Dependencies","tags":[],"description":"","content":"The system consists of two parts of source code requiring dependencies.\nStep 1: Install for Backend Navigate to the backend Lambda directory and install the required Node.js packages:\ncd backend/src/lambda npm install This will install all dependencies defined in the package.json file for the Lambda function.\nStep 2: Install for CDK Go back to the CDK root directory and install CDK dependencies:\n# Go back to CDK root directory cd ../../../cdk npm install This installs the AWS CDK libraries and constructs needed to synthesize and deploy the infrastructure.\nMake sure you have Node.js and npm installed on your machine before running these commands.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions To deploy the Findnest Backend stack securely, adhering to the principle of least privilege, add the following JSON policy to your IAM User.\nThis policy focuses strictly on the services defined in your CDK code (DynamoDB, Cognito, Location, etc.)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;FindnestDeploymentPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;iam:*\u0026#34;, \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;dynamodb:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;cognito-identity:*\u0026#34;, \u0026#34;geo:*\u0026#34;, \u0026#34;sns:*\u0026#34;, \u0026#34;bedrock:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;ssm:GetParameter\u0026#34;, \u0026#34;sts:AssumeRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources (CDK Bootstrap) In this lab, we will use N. Virginia region (us-east-1). To prepare the workshop environment, we need to provision the CDK Bootstrap resources. This process creates an S3 Bucket to store your Lambda code and CloudFormation templates.\n1. Install Dependencies\nEnsure you have the core tools installed on your local machine:\nnpm install -g aws-cdk 2. Bootstrap the Environment\nRun the following command in your terminal to deploy the bootstrap stack. Accept all of the defaults.\ncdk bootstrap aws://\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/us-east-1 (Replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your 12-digit AWS Account ID)\nWait for the CloudFormation stack CDKToolkit to reach CREATE_COMPLETE status. 1 S3 Bucket has been created to store assets. The environment is now ready for serverless deployment.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-systemvalidation/5.5.2-test-admin-login/","title":"Test Admin Login","tags":[],"description":"","content":"Test Case 1: Admin Login This is the most critical test. It verifies:\n‚úÖ API Gateway correctly routes traffic to Lambda ‚úÖ Lambda can connect to Cognito ‚úÖ Cognito validates the credentials seeded in Section 5.4 Request Details Method: POST\nURL: \u0026lt;ApiUrl\u0026gt;/auth/login\nBody (JSON):\n{ \u0026#34;email\u0026#34;: \u0026#34;admin@findnest.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Password@123\u0026#34; } Replace the email and password with the credentials you created in Section 5.4 (Seeding Data).\nExecute with cURL curl -X POST \u0026lt;YOUR_API_URL\u0026gt;/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;: \u0026#34;admin@findnest.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Password@123\u0026#34;}\u0026#39; Example:\ncurl -X POST https://xyz123.execute-api.us-east-1.amazonaws.com/prod/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;email\u0026#34;: \u0026#34;admin@findnest.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Password@123\u0026#34;}\u0026#39; Expected Response (200 OK) You should receive a JSON object containing the authentication tokens (ID Token, Access Token):\n{ \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;admin@findnest.com\u0026#34;, \u0026#34;accessToken\u0026#34;: \u0026#34;eyJraWQiOiJ...\u0026#34;, \u0026#34;refreshToken\u0026#34;: \u0026#34;eyJjdHkiOiJ...\u0026#34;, \u0026#34;idToken\u0026#34;: \u0026#34;eyJraWQiOiJ...\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Admins\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;Login successful\u0026#34; } Important Action Copy the accessToken from the response. We will need it for the next test case.\nIf you receive the tokens, congratulations! Your authentication system is working correctly.\nTroubleshooting If you encounter errors:\n400 Bad Request: Check that the JSON body is formatted correctly 401 Unauthorized: Verify the email and password match what you created in Section 5.4 500 Internal Server Error: Check Lambda logs in CloudWatch Logs Network Error: Verify the API URL is correct and accessible Using Postman If you prefer using Postman:\nCreate a new POST request Set URL to \u0026lt;YOUR_API_URL\u0026gt;/auth/login Go to Body tab ‚Üí Select raw ‚Üí Choose JSON Paste the JSON body Click Send Copy the accessToken from the response "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/5.6.2-verify-deletion/","title":"Verify Resource Deletion","tags":[],"description":"","content":"Although we configured RemovalPolicy.DESTROY and autoDeleteObjects: true in our CDK code, it is a Best Practice to double-check the AWS Console to ensure everything is gone.\nA. Check Amazon S3 Go to the S3 Console Search for findnest in the bucket list Verify the bucket findnest-images-... is deleted If the bucket still exists:\nSelect the bucket Click Empty button Confirm by typing permanently delete After emptying, click Delete button Confirm by typing the bucket name The autoDeleteObjects: true setting in our code usually handles this automatically by deploying a custom Lambda function to clear the bucket before deletion.\nB. Check DynamoDB Tables Go to the DynamoDB Console Click Tables in the left sidebar Verify that all tables are deleted: BoardingHouseListings UserProfiles OTPVerifications UserFavorites SupportRequests SearchHistory UserPreferences If any tables remain, select them and click Delete.\nC. Check CloudWatch Logs Go to the CloudWatch Console Navigate to Logs ‚Üí Log groups Search for /aws/lambda/FindNestApi If the log group persists (sometimes logs generated during deletion are kept):\nSelect the log group Choose Actions ‚Üí Delete log group(s) Confirm deletion Log groups might accumulate small charges over time. It\u0026rsquo;s good practice to delete them if no longer needed.\nD. Check Lambda Functions Go to the Lambda Console Verify that FindNestApi function is deleted If it still exists, select it and click Actions ‚Üí Delete.\nE. Check API Gateway Go to the API Gateway Console Verify that FindNestAPI is deleted If it still exists, select it and click Actions ‚Üí Delete API.\nF. Check Cognito Go to the Cognito Console Check User Pools - Verify FindNestUsers is deleted Check Identity Pools - Verify FindNestMapAccess is deleted G. Check Location Service Go to the Amazon Location Service Console Verify the following are deleted: Map: FindNestMap Place Index: FindNestPlaces Route Calculator: FindNestRoutes H. Check CloudFormation Go to the CloudFormation Console Verify that BackendStack shows status DELETE_COMPLETE or is no longer listed If you don\u0026rsquo;t see the stack or it shows DELETE_COMPLETE, your cleanup is successful!\nVerification Checklist Confirm all resources are deleted:\n‚úÖ S3 Bucket (findnest-images-*) ‚úÖ DynamoDB Tables (all 7 tables) ‚úÖ Lambda Function (FindNestApi) ‚úÖ API Gateway (FindNestAPI) ‚úÖ Cognito User Pool (FindNestUsers) ‚úÖ Cognito Identity Pool (FindNestMapAccess) ‚úÖ Location Service resources (Map, Place Index, Route Calculator) ‚úÖ CloudWatch Log Groups (/aws/lambda/FindNestApi) ‚úÖ CloudFormation Stack (BackendStack) ‚úÖ IAM Roles (auto-deleted with stack) If any resources remain after 10 minutes, manually delete them to avoid unexpected charges.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Overview This week focuses on designing, building and securing a Virtual Private Cloud (VPC) environment on AWS. The work includes network design and firewall controls, deploying and securing EC2 instances, establishing a site-to-site VPN, and automating the setup using Infrastructure-as-Code templates. Clean-up and documentation are part of the workflow to ensure reproducibility and security.\nPrerequisites An AWS account with permissions to create VPCs, EC2, VPN and related resources AWS CLI installed and configured (Access Key, Secret Key, Default Region) Basic familiarity with CloudFormation or Terraform (for IaC sections) Access to the First Cloud Journey reference materials: https://cloudjourney.awsstudygroup.com/ Note: Always follow your organization\u0026rsquo;s security policy and avoid sharing any credentials or sensitive keys. Use ephemeral resources and clean up test resources when done.\nWeek 2 Objectives: Understand VPC fundamentals and firewall controls (Security Groups \u0026amp; NACLs). Design and build a secure VPC with public/private subnets, routing and gateways. Deploy and secure EC2 instances, including SSH access and EBS usage. Configure a Site-to-Site VPN for hybrid connectivity and test routing. Practice Infrastructure-as-Code (CloudFormation/Terraform) to automate deployments. Clean up resources and document all steps and learning points. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Study VPC architecture and firewall concepts (Security Groups \u0026amp; NACLs); draw a secure VPC design plan (public/private subnets). 09/16/2025 09/16/2025 https://000003.awsstudygroup.com/ 2 Build the VPC: create VPC, subnets (public/private), route tables, Internet Gateway and NAT Gateway; implement basic Security Groups and NACL baselines. 09/17/2025 09/17/2025 https://000003.awsstudygroup.com/ 3 Deploy EC2 instances: choose AMI \u0026amp; instance type, launch EC2 within the correct subnet, configure key pairs and IAM role, attach an EBS volume. 09/18/2025 09/18/2025 https://000004.awsstudygroup.com/ 4 Harden EC2 networking \u0026amp; firewall: create Security Group rules for SSH/HTTP/HTTPS, test connectivity, assign Elastic IP (if required) and verify routing from subnet to IGW/NAT. 09/19/2025 09/19/2025 https://000004.awsstudygroup.com/ 5 Setup a site-to-site VPN: configure Customer Gateway, Virtual Private Gateway and VPN connection; verify route propagation and test end-to-end connectivity. 09/20/2025 09/20/2025 https://000003.awsstudygroup.com/ 6 Implement Infrastructure-as-Code (CloudFormation or Terraform): write templates to provision the VPC, subnets, EC2, SGs and VPN resources; test deployment from IaC templates. 09/21/2025 09/21/2025 https://000037.awsstudygroup.com/ 7 Clean up all test resources safely (terminate instances, detach \u0026amp; snapshot volumes if needed, remove VPC), and write a short summary of lessons learned plus documentation for reproducibility. 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Designed and provisioned a secure VPC (public \u0026amp; private subnets, IGW, NAT) following best practices. Implemented Security Groups and NACLs to enforce network-level firewall controls for EC2 and subnet traffic. Deployed EC2 instances using appropriate AMIs and instance types, attached EBS storage, and configured SSH access. Configured and validated a Site-to-Site VPN connection, verified routing and basic connectivity between networks. Created Infrastructure-as-Code templates (CloudFormation/Terraform) to automate VPC, EC2, and VPN provisioning. Cleaned up test resources and documented the deployment steps, templates, and lessons learned for reproducibility. "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated blog posts","tags":[],"description":"","content":" List of translated posts 1) Proposed Project: Serverless Photo Analytics \u0026amp; Moderation 2025-09-17\nSummary: A project to build a serverless photo album that supports user sign-in, presigned uploads to S3, automatic labeling and moderation (Rekognition), search by labels, and global delivery via CloudFront ‚Äî with observability and cost alerts for demo/teaching use.\n2) Project Proposal: Intelligent Web Security Solution on AWS 2025-12-01\nSummary: Design and implementation plan for an automated web application security stack using AWS WAF/Shield, CloudFront/ALB, CloudWatch anomaly detection, Lambda/Athena log processing, and optional SageMaker models for advanced anomaly detection.\n3) Project Proposal: AI-Driven Threat Detection and Mitigation on AWS 2025-11-25\nSummary: An automated monitoring‚Üídetection‚Üíresponse architecture using GuardDuty, EventBridge, and Lambda for fast remediation, plus an optional SageMaker-based anomaly detection pipeline for enhanced threat detection; focuses on cost-effective serverless design for student projects.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"AWS Cloud Mastery Series #3 ‚Äî Well‚ÄëArchitected Security Pillar\nDate: Saturday, 29 November 2025 | 08:30 - 12:00 (GMT+7)\nLocation: Bitexco Financial Tower, District 1, Ho Chi Minh City\nPersonal Reflection ‚Äî Security Pillar Workshop Attending the AWS Well‚ÄëArchitected: Security Pillar session gave me a structured and practical view of cloud security. The workshop covered five main areas ‚Äî IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response ‚Äî and tied them together with real examples and recommended practices.\nKey learnings I took away:\nIdentity \u0026amp; Access Management: Reinforced the importance of avoiding long‚Äëterm credentials, using roles and short‚Äëlived credentials, adopting SSO through IAM Identity Center, and applying SCPs/permission boundaries for multi‚Äëaccount setups. Detection \u0026amp; Logging: Emphasized comprehensive logging (CloudTrail at org level, VPC Flow Logs, ALB/S3 logs), enabling GuardDuty and Security Hub, and using EventBridge for automated alerting. The idea of Detection‚Äëas‚ÄëCode stood out as a practical way to version and test detection rules. Network \u0026amp; Workload Security: Covered VPC segmentation, private vs public placement, pragmatic use of Security Groups vs NACLs, and protections like WAF, Shield, and Network Firewall to reduce the attack surface. Data Protection: Clear patterns for encryption at rest and in transit using KMS, key policies and rotation, plus using Secrets Manager/Parameter Store for secrets management and automated rotation. Incident Response (IR): Walked through IR playbooks for common scenarios (compromised IAM key, accidental S3 public exposure, EC2 malware). I appreciated the actionable steps: isolate, snapshot, collect evidence, and automate response using Lambda/Step Functions. What impressed me most:\nThe practicality of IR playbooks ‚Äî they were not theoretical but included clear steps and tools to execute during an incident. Detection‚Äëas‚ÄëCode: treating detection logic like application code enables reviews, testing, and CI/CD for security rules. Multi‚Äëaccount IAM design: examples of permission boundaries and SCPs made the least‚Äëprivilege principle tangible at scale. How I\u0026rsquo;ll apply this right away:\nAudit IAM in my projects: remove long‚Äëterm credentials, enforce MFA, and review permission scopes for roles. Ensure CloudTrail is enabled at the organization level and centralize logs for analysis and retention. Draft two IR playbooks (compromised key and S3 public exposure) with explicit steps and scripts for evidence collection. Prototype a Detection‚Äëas‚ÄëCode workflow: store detection rules in a repo and deploy them via CI to a test account. Conclusion:\nThis workshop connected high‚Äëlevel security principles to actionable practices I can start implementing immediately. It gave me a clear to‚Äëdo list to improve the security posture of small projects within the next few weeks.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Project Proposal: AI-Driven Threat Detection and Mitigation on AWS Objective: Build an automated system that detects and responds to security threats in an AWS environment using AWS security services combined with AI/ML to continuously monitor, detect anomalies, and automatically mitigate threats. This project helps students learn cloud security, the application of AI in defense, and how to leverage the AWS Free Tier to control costs.\nIntroduction and Context Modern cloud environments generate large volumes of logs and security events. Automating monitoring and response is essential because manual operations cannot react quickly enough. AWS recommends automating security response to detect and remediate incidents faster.[1][2] For example, Amazon GuardDuty continuously analyzes CloudTrail, VPC Flow Logs, and DNS logs‚Äîusing ML and threat intelligence to identify suspicious or malicious activity.[3][4] When GuardDuty raises a finding, the system can automatically listen for those events via EventBridge/CloudWatch and trigger remediation code (for example, AWS Lambda) to quickly remediate (detach connections, disable credentials, etc.).[5][6]\nThe proposed approach follows the monitoring ‚Üí detection ‚Üí automated response model: monitoring components collect logs (CloudTrail, VPC Flow Logs, EC2 metrics, etc.)[7]. When an abnormal condition occurs (e.g., a GuardDuty finding or a CloudWatch alarm), the system triggers an automated prevention action (lock a user, block an IP, notify admins).[6][5]\nFigure: Automated monitoring ‚Üí detection ‚Üí response flow on AWS (source: AWS Security Blog).\nProposed System Architecture Detection: Use Amazon GuardDuty to monitor suspicious activity across the AWS account (analysing CloudTrail, VPC Flow Logs, DNS logs). GuardDuty is a managed service that uses ML to identify threats without requiring you to author many rules.[3][4] Event Processing \u0026amp; Response: Configure Amazon EventBridge (CloudWatch Events) to listen for GuardDuty findings. When a finding occurs, EventBridge triggers an AWS Lambda function (Node.js runtime) to execute remediation logic. The Lambda function can send notifications via Amazon SNS (email/SMS), write detailed records to logs, or call APIs to disable users, update NACLs/security groups, etc. This serverless design minimizes cost and simplifies deployment. Automated Remediation: For example, if GuardDuty detects unauthorized access, Lambda could revoke access keys, add the offending IP to a blocked list on a Network ACL, or disable a compromised API key. All actions should be recorded in CloudWatch Logs and forwarded to SNS for admin visibility.[6] Monitoring \u0026amp; Alerting: Use CloudWatch to aggregate logs from GuardDuty, Lambda, and CloudTrail. CloudWatch Dashboards or Logs Insights can visualize security metrics. SNS sends immediate alerts to administrators. Optionally, a small management UI (Node.js/Express) can display summaries, though relying on CloudWatch/SNS reduces operations overhead. ML/AI Model: To enhance detection capabilities, you can use Amazon SageMaker for custom anomaly detection models. For example, collect logs (CloudWatch, VPC logs) into S3, then train anomaly detection models (e.g., Random Cut Forest). Use SageMaker Processing and Pipelines for preprocessing, training, and model selection.[8] For an initial two-month implementation, prioritize built-in tools (GuardDuty and rule-based detection) and incrementally add ML components. SageMaker has a Free Tier (250 hours of ml.t3.medium notebook usage per month for two months), enough for basic model experiments.[9]\nImplementation stack: Core remediation code written in Node.js. Lambda functions use the AWS SDK to act on resources (SNS, IAM, EC2 APIs, etc.). If a management UI is needed later, deploy a small Node/Express app (EC2/ECS/EKS), but prefer serverless to reduce ops burden and cost. Cost and Optimization By leveraging serverless services and Free Tier offerings, the project can be cost-effective:\nGuardDuty: 30-day free trial is available per account.[10] After the trial, cost is based on analyzed log volume (CloudTrail, VPC Flow, DNS). For a small demo account, costs can be a few USD/month if log volume is low. Consider enabling GuardDuty only in non-production/demo accounts to control cost. Lambda \u0026amp; SNS: Lambda includes 1M free requests and 400,000 GB-seconds per month; SNS includes 1M notifications per month, so small-scale demo workloads usually fall within free quotas.[11] CloudWatch: CloudWatch Logs provides a 5 GB ingestion free tier; costs rise only when logs exceed the free allowance. For modest log volumes (\u0026lt;5 GB), cost is near zero; larger volumes (e.g., 30 GB) may cost ~12.50 USD/month for the excess.[12] SageMaker: The Free Tier provides 250 hours/month of ml.t3.medium notebook usage for the first two months. Heavy training workloads will increase cost, so limit large training runs for demos. Estimated overall cost: With serverless design and Free Tier use, operating costs should be in the low tens of USD/month (or less) for prototype/demo scale. Careful cleanup of resources after testing prevents unexpected charges.\nBenefits and Value Educational and practical: Students learn cloud security, AWS security services (GuardDuty, Security Hub, CloudTrail), and automated response architectures, with optional ML integration for advanced detection.[2][5][8] Low cost: Free Tier and serverless services make the project accessible to students with minimal cost overhead. High applicability: The prototype can be extended to production-grade monitoring for real AWS accounts, demonstrating feasibility to evaluators. Automation: Automated detection and remediation reduce time-to-remediate and lower the risk of severe incidents due to slow manual response.[5][1] 2-Month Implementation Plan Weeks 1‚Äì2: Research \u0026amp; Design. Study GuardDuty, CloudWatch, EventBridge, Lambda, and SNS. Define event flows and IAM permissions. Produce architecture diagrams. Weeks 3‚Äì4: Basic Platform Implementation. Enable GuardDuty and CloudTrail to generate logs. Implement a simple Lambda (Node.js) to process GuardDuty findings (e.g., send SNS notifications). Create EventBridge rules for GuardDuty findings and test in a sandbox account. Weeks 5‚Äì6: Feature Expansion. Add additional rules (e.g., CloudWatch Logs anomaly detection), configure SNS email alerts, and build a basic report using CloudWatch Dashboards or a small API. Begin collecting logs to S3 for ML. Weeks 7‚Äì8: ML Model \u0026amp; Tuning. Use SageMaker Studio/Notebooks to train a log anomaly detection model (e.g., Random Cut Forest). Build a simple pipeline to score logs; integrate ML results into Lambda triggers if time permits. Test end-to-end and optimize costs (delete unused resources). Finalization: Prepare the final report and presentation. Clean up AWS resources to avoid ongoing charges. Conclusion This proposed system automates detection and remediation of security incidents on AWS using a serverless architecture and Node.js. Adding ML via SageMaker is an advanced extension that increases the academic value of the project. The approach is feasible within 2 months and leverages AWS Free Tier to minimize cost, producing a prototype demonstrating automated monitoring ‚Üí detection ‚Üí response.\nSources: Concepts and technologies referenced from AWS documentation and blog posts (GuardDuty, Lambda pricing, CloudWatch, SageMaker).[4][3][11][12][8][10]\n[1] https://aws.amazon.com/blogs/security/how-get-started-security-response-automation-aws/ [2] https://aws.plainenglish.io/automating-real-time-threat-detection-monitoring-and-response-in-aws-b79b3895b99f [3] https://medium.com/@dyavanapellisujal7/serverless-incident-response-workflow-on-aws-using-guardduty-eventbridge-sns-lambda-b24914b96581 [4] https://aws.amazon.com/guardduty/ [5] https://aws.amazon.com/blogs/security/how-get-started-security-response-automation-aws/ [6] https://aws.amazon.com/blogs/security/how-get-started-security-response-automation-aws/ [7] https://aws.amazon.com/blogs/security/how-get-started-security-response-automation-aws/ [8] https://aws.amazon.com/blogs/machine-learning/efficiently-build-and-tune-custom-log-anomaly-detection-models-with-amazon-sagemaker/ [9] https://aws.amazon.com/pm/sagemaker/ [10] https://aws.amazon.com/blogs/security/how-get-started-security-response-automation-aws/ [11] https://aws.amazon.com/lambda/pricing/ [12] https://aws.amazon.com/cloudwatch/pricing/\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-deploybackend/","title":"Deploy Backend","tags":[],"description":"","content":"Introduction to Infrastructure as Code (IaC) Instead of manually clicking through the AWS Console (\u0026ldquo;ClickOps\u0026rdquo;), we use AWS CDK to define our entire infrastructure. In the file cdk/lib/backend-stack.ts, we have designed a complete Serverless system.\nWhen you run the cdk deploy command, CDK synthesizes this code into a CloudFormation Template, and AWS automatically provisions the corresponding resources.\nContent Architecture Deep Dive Install Dependencies Deploy Stack Results \u0026amp; Outputs "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-deploybackend/5.3.3-deploy-stack/","title":"Deploy Stack","tags":[],"description":"","content":"Now, let\u0026rsquo;s turn this design into reality.\nStep 1: Synthesize Run the synthesize command to generate the CloudFormation template:\ncdk synth This command generates the CloudFormation template in the cdk.out directory. If no red errors appear, you may proceed.\nThe synthesize process converts your CDK code (TypeScript) into a CloudFormation template (JSON/YAML).\nStep 2: Deploy Deploy the stack to AWS:\ncdk deploy --require-approval never The --require-approval never flag bypasses the confirmation prompt for IAM changes.\nThe deployment process will take approximately 5-10 minutes to provision all resources.\nDuring deployment, you\u0026rsquo;ll see:\nCloudFormation stack creation progress Individual resources being created (DynamoDB tables, Lambda functions, API Gateway, etc.) Status updates for each resource Wait for the deployment to complete successfully before proceeding to the next step.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-seedingdata/5.4.3-run-script/","title":"Run the Script","tags":[],"description":"","content":"Now, execute the script. It is interactive, so it will ask you for details.\nExecute the script node seed-admin.js Interactive Walkthrough The script will prompt you for the following information:\nUsername: Enter a username (e.g., superadmin) Password: Enter a strong password (min 8 chars, uppercase, lowercase, numbers, special chars) Email: Enter a valid email address Full Name: Enter a display name Confirm: Type yes to proceed Expected Output If everything is configured correctly, you should see output similar to this:\n‚úÖ Environment variables loaded: Region: us-east-1 User Pool ID: us-east-1_xxxxxx User Profiles Table: UserProfiles üìù Creating admin user in Cognito... ‚úÖ User created with ID: xxxx-xxxx-xxxx üîë Setting permanent password... ‚úÖ Password set üëë Adding user to Admins group... ‚úÖ Added to Admins group üìù Creating admin profile in DynamoDB... ‚úÖ Profile created ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ‚úÖ ADMIN USER CREATED SUCCESSFULLY! ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê Save the credentials! You\u0026rsquo;ll need the username and password to log in to the system.\nIf you encounter errors:\nCheck that your .env file has the correct values Verify your AWS credentials are configured (aws configure) Ensure you have the necessary IAM permissions Make sure the CDK stack was deployed successfully What the script does The script performs the following operations:\n‚úÖ Creates a user in Cognito User Pool with the provided username and password ‚úÖ Sets the password as permanent (no need to change on first login) ‚úÖ Adds the user to the \u0026ldquo;Admins\u0026rdquo; group for elevated permissions ‚úÖ Creates a profile record in DynamoDB with user information "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-systemvalidation/5.5.3-test-create-listing/","title":"Test Create Listing","tags":[],"description":"","content":"Test Case 2: Create a Listing (Protected Route) Now, let\u0026rsquo;s test if the permissions are working. Only authenticated users (Landlords/Admins) can create a boarding house listing.\nThis tests Lambda\u0026rsquo;s connection to DynamoDB.\nRequest Details Method: POST\nURL: \u0026lt;ApiUrl\u0026gt;/landlord/listings\nHeaders:\nAuthorization: Bearer \u0026lt;YOUR_ACCESS_TOKEN\u0026gt; Content-Type: application/json Body (JSON):\n{ \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Full furniture, near manufacturing hub\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi, District 1, HCMC\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;, \u0026#34;Parking\u0026#34;] } Replace \u0026lt;YOUR_ACCESS_TOKEN\u0026gt; with the token you copied from the previous test (Admin Login).\nExecute with cURL curl -X POST \u0026lt;YOUR_API_URL\u0026gt;/landlord/listings \\ -H \u0026#34;Authorization: Bearer \u0026lt;PASTE_TOKEN_HERE\u0026gt;\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Full furniture\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;] }\u0026#39; Example:\ncurl -X POST https://xyz123.execute-api.us-east-1.amazonaws.com/prod/landlord/listings \\ -H \u0026#34;Authorization: Bearer eyJraWQiOiJxxx...\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Full furniture, near manufacturing hub\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi, District 1, HCMC\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;, \u0026#34;Parking\u0026#34;] }\u0026#39; Expected Response (201 Created) { \u0026#34;message\u0026#34;: \u0026#34;Listing created successfully\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;listingId\u0026#34;: \u0026#34;listing_12345...\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi, District 1, HCMC\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;, \u0026#34;Parking\u0026#34;], \u0026#34;createdAt\u0026#34;: \u0026#34;2023-10-27T10:30:00Z\u0026#34; } } If you receive a 201 Created response with the listing data, your protected route and DynamoDB integration are working correctly!\nWhat This Test Validates ‚úÖ JWT Token Verification: Lambda validates the access token ‚úÖ Authorization: Only authenticated users can create listings ‚úÖ DynamoDB Write: Lambda successfully writes data to DynamoDB ‚úÖ Data Persistence: The listing is stored in the database Troubleshooting Common errors:\n401 Unauthorized: Token is invalid or expired. Login again to get a fresh token 403 Forbidden: User doesn\u0026rsquo;t have the required permissions 400 Bad Request: Check the JSON body format and required fields 500 Internal Server Error: Check Lambda logs for DynamoDB permission issues Using Postman If using Postman:\nCreate a new POST request Set URL to \u0026lt;YOUR_API_URL\u0026gt;/landlord/listings Go to Headers tab: Add Authorization: Bearer \u0026lt;YOUR_TOKEN\u0026gt; Add Content-Type: application/json Go to Body tab ‚Üí Select raw ‚Üí Choose JSON Paste the JSON body Click Send "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/5.6.3-conclusion/","title":"Workshop Conclusion","tags":[],"description":"","content":"Congratulations You have successfully completed the Findnest Serverless Backend Workshop!\nWhat Have You Achieved? 1. Modern Architecture\nYou deployed a fully Serverless architecture using:\n‚úÖ AWS Lambda - Serverless compute for backend logic ‚úÖ API Gateway - RESTful API endpoint management ‚úÖ DynamoDB - NoSQL database with on-demand scaling ‚úÖ Amazon Cognito - User authentication and authorization ‚úÖ Amazon Location Service - Maps and geocoding capabilities ‚úÖ Amazon Bedrock - AI/ML integration with Claude 3 ‚úÖ Amazon SNS - SMS notifications for OTP ‚úÖ Amazon S3 - Object storage for images 2. Infrastructure as Code\nYou used AWS CDK (TypeScript) to:\nDefine and provision cloud resources professionally Avoid manual \u0026ldquo;ClickOps\u0026rdquo; errors Enable version control for infrastructure Deploy and destroy entire stacks with single commands Implement proper resource dependencies 3. Advanced Integration\nYou integrated specialized services:\nAuthentication: Multi-factor auth with phone verification Database: 7 DynamoDB tables with proper indexes and TTL Storage: Secure S3 buckets with automatic cleanup Geolocation: Maps, place search, and route calculation AI/ML: Claude 3 for intelligent recommendations Notifications: SMS via SNS for verification codes 4. Operational Best Practices\nYou applied:\n‚úÖ Least Privilege Permissions - Granular IAM policies ‚úÖ Automated Cleanup - RemovalPolicy and autoDeleteObjects ‚úÖ Environment Variables - Configuration management ‚úÖ Logging - CloudWatch integration ‚úÖ CORS Configuration - Secure cross-origin requests ‚úÖ API Throttling - Rate limiting protection Skills Acquired By completing this workshop, you now have hands-on experience with:\nServerless Architecture Design\nEvent-driven patterns Stateless compute Managed services integration AWS CDK Development\nTypeScript constructs Resource provisioning Stack management API Development\nRESTful endpoints Authentication flows Protected and public routes Database Design\nNoSQL data modeling DynamoDB best practices TTL and indexes Security Implementation\nIAM policies Cognito user pools JWT token validation DevOps Practices\nInfrastructure as Code Automated deployments Resource cleanup Next Steps Continue your learning journey:\nEnhance the Backend:\nAdd more features (reviews, payments, real-time chat) Implement caching with Amazon ElastiCache Add API versioning Set up CloudWatch alarms and monitoring Implement CI/CD pipeline with AWS CodePipeline Build the Frontend:\nCreate a React Native mobile app Integrate with the API you just built Implement map visualization Add real-time notifications Production Readiness:\nSet up multiple environments (dev, staging, prod) Implement secrets management with AWS Secrets Manager Add comprehensive error handling Set up automated testing Configure custom domain with Route 53 Learn More AWS Services:\nAWS AppSync for GraphQL APIs Amazon EventBridge for event-driven architecture AWS Step Functions for workflow orchestration Amazon SQS for message queuing Resources AWS CDK Documentation AWS Lambda Best Practices DynamoDB Design Patterns API Gateway Documentation Thank You You are now equipped with the knowledge to build scalable, secure, and cost-effective backends on AWS.\nHappy Building! üöÄ\nKeep experimenting, keep learning, and keep building amazing things with AWS!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Introduce Amazon Elastic Compute Cloud (EC2), including instance types, AMIs, and storage options. Prepare and provision network infrastructure (VPCs) and firewall policies for Linux and Windows workloads. Launch and configure Microsoft Windows Server 2022 and Amazon Linux instances. Manage EC2 instance lifecycle (modify configuration, snapshots, AMIs, and recovery options). Deploy sample applications (User Management App and Node.js app) for real-world validation. Apply IAM best practices for cost \u0026amp; usage governance, and document cleanup procedures. Introduction: What is Amazon EC2? Amazon Elastic Compute Cloud (EC2) is a web service that provides secure, resizable compute capacity in the cloud. EC2 instances are virtual servers you can use to run applications. Key concepts covered this week:\nAmazon EC2 Instance Types: General purpose, compute-optimized, memory-optimized, etc. Amazon Machine Images (AMI): Pre-configured OS and application templates used to launch instances. Storage Options: EBS (Elastic Block Store), Instance store, EBS-backed AMIs. Reference: https://000004.awsstudygroup.com/\nPreparation (follow Module 2: Prerequisite) Follow the EC2 workshop prerequisite module for all preparation steps. Key items include:\nEnsure you have an AWS account and a default VPC or prepare a VPC/subnet for the lab as instructed in Module 2. Create Security Groups required for Linux and Windows instances (SSH, RDP, HTTP, HTTPS as needed). Create or import Key Pairs for Linux and confirm retrieval method for the Windows Server admin password. Create an IAM role for EC2 with SSM permissions and any S3 access required for snapshot or backup operations. Reference: https://000004.awsstudygroup.com/2-prerequiste\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Introduction \u0026amp; Preparation: Learn EC2 types/AMIs/storage; create VPCs and subnets; create Key Pairs and IAM role for EC2; design security group rules for Linux and Windows instances. 09/23/2025 09/23/2025 https://000004.awsstudygroup.com/ 2 Preparation \u0026amp; Network: Follow the EC2 prerequisite module to prepare network settings, security groups for Linux (SSH/HTTP/HTTPS) and Windows (RDP/HTTP/HTTPS), and key pair/credentials. 09/24/2025 09/24/2025 https://000004.awsstudygroup.com/2-prerequiste 3 Harden security and verify connectivity: Configure Security Groups, attach Key Pairs, and validate access (RDP/SSH) to prepared instances. 09/25/2025 09/25/2025 https://000004.awsstudygroup.com/2-prerequiste 4 Launch Microsoft Windows Server 2022 instance: Launch Windows Server 2022 AMI, configure admin password, add to Security Group, assign Elastic IP and connect via RDP. 09/26/2025 09/26/2025 https://000004.awsstudygroup.com/ 5 Launch Amazon Linux instance: Launch Amazon Linux 2 AMI, attach Key Pair, configure Security Group for SSH; connect via SSH and attach an EBS volume for data. 09/27/2025 09/27/2025 https://000004.awsstudygroup.com/ 6 EC2 Basics \u0026amp; Hardening: Modify instance configuration (instance type, user-data), create and manage EBS snapshots, build custom AMIs (optional), test recovery via Systems Manager and user-data. 09/28/2025 09/28/2025 https://000004.awsstudygroup.com/ 7 Deploy Applications \u0026amp; Governance: Deploy AWS User Management App on Amazon Linux 2; deploy Node.js app on EC2 Windows; implement cost and usage governance with IAM roles and resource tags; cleanup and document steps. 09/29/2025 09/29/2025 https://000004.awsstudygroup.com/9-cleanup/ Launch Microsoft Windows Server 2022 Instance Create or select a Windows Server 2022 AMI (Region-specific). Choose instance type as required (e.g., t3.medium for testing). Select the Windows security group that allows RDP (3389), HTTP (80) and HTTPS (443). Attach an EBS volume and (optionally) assign an Elastic IP for remote access. Retrieve the administrator password via EC2 console and connect via RDP. Launch Amazon Linux Instance Launch Amazon Linux 2 AMI with an appropriate instance type (e.g., t3.micro for testing). Attach the Linux security group enabling SSH (22), HTTP (80) and HTTPS (443). Connect using the Key Pair and SSH, and verify network connectivity. Attach and mount an additional EBS volume for data storage. Amazon EC2 Basic Operations Modify EC2 Instance Configuration: Change instance types, add or remove EBS volumes, and edit instance tags. Create and Manage EBS Snapshots: Snapshot volumes before major changes, store snapshots in separate S3 buckets or AMIs. Build Custom AMIs: After configuring an instance, create an AMI for repeatable deployments. Launch Instances from Custom AMIs: Validate the AMI by spinning up a new instance from it. Recover Access to Windows Instances Using AWS Systems Manager (SSM): Use SSM Session Manager or EC2Rescue (if SSM is enabled). Recover Access to Linux Instances Using User Data: Use cloud-init user-data scripts or attach a recovery instance to mount the root volume and fix SSH configuration. Configure GUI Desktop Environment for Ubuntu (Optional): Install and configure a lightweight desktop (e.g., XFCE) for testing or administrative convenience. Implement EBS Volume Archiving: Use snapshot lifecycle, move older snapshots to lower-cost storage, or export to S3 for long-term retention. Deploying Applications Deploying an AWS User Management Application on Amazon Linux 2:\nPrepare the instance, install dependencies (e.g., Node.js, Nginx, database client), and clone the user management app from a repo. Configure environment variables, start the service (systemd), and confirm connectivity using the public IP or a load balancer. Deploying Node.js Applications on Amazon EC2 Windows:\nInstall Node.js and required runtime tools on Windows Server 2022. Deploy the application, configure IIS or a reverse-proxy like Nginx for Windows, and test the endpoint. Cost \u0026amp; Usage Governance with IAM Create IAM users with least-privilege policies for day-to-day operations. Use IAM roles for EC2 instances to avoid embedding credentials. Use resource tags to segregate test and dev resources for cost allocation. Monitor costs using AWS Cost Explorer and create budgets \u0026amp; alerts for accounts. Week 3 Achievements: Gained a practical understanding of Amazon EC2, instance types, AMIs and storage options. Created VPCs and applied carefully designed Security Groups to protect both Linux and Windows workloads. Launched and connected to Microsoft Windows Server 2022 and Amazon Linux 2 instances; attached EBS volumes and verified I/O. Performed EBS snapshot operations and created a custom AMI for repeatable deployments. Validated instance recovery methods using AWS Systems Manager and user-data-based recovery workflows. Deployed a sample User Management app on Amazon Linux 2 and a Node.js app on Windows for functional testing. Implemented IAM best practices and resource tagging to support cost \u0026amp; usage governance. Documented all steps with commands, templates, and learnings for reproducibility. Quick Reference / Commands aws ec2 describe-instances --output table aws ec2 create-snapshot --volume-id vol-xxxxxxxx --description \u0026#34;pre-change snapshot\u0026#34; aws ec2 create-image --instance-id i-xxxxxxxx --name \u0026#34;Custom-AMI-Week3\u0026#34; --no-reboot aws ec2 run-instances --image-id ami-xxxxxxxx --count 1 --instance-type t3.micro --key-name my-key --security-group-ids sg-xxxxxxxx --subnet-id subnet-xxxxxxxx aws ec2 create-tags --resources i-xxxxxxxx --tags Key=Project,Value=Week3-EC2 References: Amazon EC2 overview: https://000004.awsstudygroup.com/ EC2 workshop home: https://000004.awsstudygroup.com/ "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"This section lists the events I attended during the internship period. Each entry links to a detailed report (reflection, highlights, and lessons learned) maintained in its event page.\nBelow is a concise summary of the events I participated in:\nEvent 1 Title: AI/ML \u0026amp; Generative AI on AWS (AWS Cloud Mastery Series #1) Date: 15 November 2025 Location: Bitexco Financial Tower, Ho Chi Minh City Role: Attendee Summary: Practical overview of Amazon SageMaker and Amazon Bedrock, prompt engineering, and RAG patterns; included a live demo building a generative chatbot and guidance for safe deployment. Event 2 Title: DevOps on AWS (AWS Cloud Mastery Series #2) Date: 17 November 2025 Location: Bitexco Financial Tower, Ho Chi Minh City Role: Attendee Summary: End‚Äëto‚Äëend CI/CD pipelines with CodeCommit/CodeBuild/CodeDeploy/CodePipeline, Infrastructure as Code (CloudFormation/CDK), container services (ECR/ECS/EKS/App Runner) and observability using CloudWatch and X‚ÄëRay. Event 3 Title: Well‚ÄëArchitected: Security Pillar (AWS Cloud Mastery Series #3) Date: 29 November 2025 Location: Bitexco Financial Tower, Ho Chi Minh City Role: Attendee Summary: Focus on IAM best practices, detection and logging (CloudTrail, GuardDuty), network/workload protection, data encryption (KMS), secrets management and incident response playbooks with automation. Event 4 Title: Vietnam Cloud Day 2025 ‚Äî HCMC Connect Edition Date: 18 September 2025 Location: AWS Vietnam (Ho Chi Minh City) Role: Attendee Summary: Plenary and breakout tracks covering Generative AI, Data Analytics, Migration \u0026amp; Modernization, AI‚ÄëDriven Development Lifecycle, and Security; included local case studies and practical roadmaps for prototyping. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-deploybackend/5.3.4-results/","title":"Results &amp; Outputs","tags":[],"description":"","content":"Upon successful deployment, the terminal will display the Outputs section in green. These are critical parameters for connecting the Frontend to the Backend.\nStack Outputs ‚ö†Ô∏è PLEASE COPY AND SAVE THESE VALUES:\n‚úÖ BackendStack Outputs: BackendStack.ApiUrl = https://xyz123.execute-api.us-east-1.amazonaws.com/prod/ BackendStack.UserPoolId = us-east-1_AbCdEfGh BackendStack.UserPoolClientId = 1a2b3c4d5e6f7g8h9i0j BackendStack.IdentityPoolId = us-east-1:12345678-abcd-1234-abcd-1234567890ab BackendStack.ImagesBucket = findnest-images-123456789012 BackendStack.MapName = FindNestMap-123456789012 BackendStack.PlaceIndexName = FindNestPlacesV3-123456789012 BackendStack.RouteCalculatorName = FindNestRoutesV3-123456789012 BackendStack.Region = us-east-1 ‚úÖ MonitoringStack Outputs: MonitoringStack.AlertTopicArn = arn:aws:sns:us-east-1:123456789012:BoardingHouseAlerts MonitoringStack.DashboardUrl = https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=SmartBoardingHouse-Monitoring Output Parameters Explained Parameter Description Usage ApiUrl The endpoint for the Backend API Used by Frontend to make API calls UserPoolId Cognito User Pool identifier Used for user authentication (Login/Register) UserPoolClientId Cognito App Client identifier Used for user authentication (Login/Register) IdentityPoolId Cognito Identity Pool identifier Used for displaying maps on the Frontend ImagesBucket S3 bucket name for images Used for storing and retrieving room images MapName Location Service map name Used for displaying maps on the Frontend application PlaceIndexName Location Service place index Used for geocoding and search functionality RouteCalculatorName Location Service route calculator Used for calculating routes between locations Region AWS region The region where resources are deployed AlertTopicArn SNS Topic ARN for alerts Subscribe to this topic to receive system alerts DashboardUrl CloudWatch Dashboard URL Access monitoring dashboard to view system metrics Next Steps Save these outputs in a secure location. You will need them to configure the Frontend application in the next section.\nYou can also retrieve these outputs later by running cdk deploy again or checking the CloudFormation stack outputs in the AWS Console.\nCongratulations You have successfully deployed a modern Serverless Backend system, integrated with:\n‚úÖ AI (Amazon Bedrock with Claude 3) ‚úÖ Digital Maps (Amazon Location Service) ‚úÖ Strict security permissions (IAM) ‚úÖ Scalable database (DynamoDB) ‚úÖ User authentication (Cognito) ‚úÖ RESTful API (API Gateway + Lambda) ‚úÖ Comprehensive monitoring (CloudWatch Dashboard \u0026amp; Alarms) ‚úÖ Real-time alerts (SNS Notifications) "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-seedingdata/","title":"Seeding Data","tags":[],"description":"","content":"Why seed data? Your infrastructure is ready, but the database is empty. You need a \u0026ldquo;Super Admin\u0026rdquo; account to manage the system.\nWe will run a Node.js script that connects directly to AWS to:\nCreate a User in Cognito User Pool Add that user to the Admins Group Create a corresponding profile in the DynamoDB UserProfiles table Content Setup the Script Configure Environment Run the Script Verification "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-systemvalidation/5.5.4-test-public-access/","title":"Test Public Access","tags":[],"description":"","content":"Test Case 3: Public Access (Listings) Finally, verify that public users can view listings without logging in.\nThis validates that your API has both protected and public routes working correctly.\nRequest Details Method: GET\nURL: \u0026lt;ApiUrl\u0026gt;/listings\nHeaders: None required (public endpoint)\nExecute with cURL curl \u0026lt;YOUR_API_URL\u0026gt;/listings Example:\ncurl https://xyz123.execute-api.us-east-1.amazonaws.com/prod/listings Expected Response (200 OK) You should see an array containing the listing you just created in Test Case 2:\n{ \u0026#34;data\u0026#34;: [ { \u0026#34;listingId\u0026#34;: \u0026#34;listing_12345...\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Luxury Apartment in District 1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Full furniture, near manufacturing hub\u0026#34;, \u0026#34;price\u0026#34;: 5000000, \u0026#34;address\u0026#34;: \u0026#34;123 Le Loi, District 1, HCMC\u0026#34;, \u0026#34;area\u0026#34;: 45, \u0026#34;amenities\u0026#34;: [\u0026#34;Wifi\u0026#34;, \u0026#34;AC\u0026#34;, \u0026#34;Parking\u0026#34;], \u0026#34;createdAt\u0026#34;: \u0026#34;2023-10-27T10:30:00Z\u0026#34;, \u0026#34;landlordId\u0026#34;: \u0026#34;...\u0026#34; } ], \u0026#34;message\u0026#34;: \u0026#34;Listings retrieved successfully\u0026#34; } If you can see the listing without authentication, your public API route is working correctly!\nWhat This Test Validates ‚úÖ Public Access: Unauthenticated users can view listings ‚úÖ DynamoDB Read: Lambda successfully reads data from DynamoDB ‚úÖ Data Retrieval: Previously created data is accessible ‚úÖ API Routing: Both protected and public routes coexist properly Additional Tests You can also test with query parameters:\nSearch by location:\ncurl \u0026#34;\u0026lt;YOUR_API_URL\u0026gt;/listings?location=District%201\u0026#34; Filter by price range:\ncurl \u0026#34;\u0026lt;YOUR_API_URL\u0026gt;/listings?minPrice=3000000\u0026amp;maxPrice=6000000\u0026#34; Pagination:\ncurl \u0026#34;\u0026lt;YOUR_API_URL\u0026gt;/listings?page=1\u0026amp;limit=10\u0026#34; Using a Browser Since this is a GET request, you can simply open the URL in your web browser:\nhttps://xyz123.execute-api.us-east-1.amazonaws.com/prod/listings The browser will display the JSON response directly.\nTroubleshooting 404 Not Found: Verify the endpoint path is correct (/listings not /listing) Empty Array: No listings in database yet, create one first (Test Case 2) 500 Internal Server Error: Check Lambda logs for DynamoDB read permission issues CORS Error (in browser): Check API Gateway CORS configuration Use a browser extension like JSON Formatter for better readability of JSON responses in the browser.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-seedingdata/5.4.4-verification/","title":"Verification","tags":[],"description":"","content":"Let\u0026rsquo;s ensure the user exists in both the Authentication service and the Database.\nA. Check Cognito (Authentication) Go to Amazon Cognito Console Select User Pool FindNestUsers Go to Users tab You should see your superadmin with status Confirmed.\nClick on the user and verify they are in the Admins group.\nThe user should have:\nStatus: CONFIRMED (not FORCE_CHANGE_PASSWORD) Group membership: Admins B. Check DynamoDB (Data) Go to DynamoDB Console Select the table UserProfiles Click Explore table items Search for the item where userId matches the ID from the script output.\nYou should see the profile data with userType: \u0026quot;admin\u0026quot;.\nVerification Checklist Confirm the following before proceeding:\n‚úÖ User exists in Cognito User Pool with status CONFIRMED ‚úÖ User is a member of the Admins group ‚úÖ User profile exists in DynamoDB UserProfiles table ‚úÖ Profile has userType set to \u0026quot;admin\u0026quot; ‚úÖ All user attributes (email, fullName) are correctly populated Next Steps You are now ready to log in and test the system!\nYour Super Admin account is ready to use. You can now authenticate with the API using these credentials.\nKeep your admin credentials secure. Consider using AWS Secrets Manager for production environments.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.4-evemt4/","title":"Event 4","tags":[],"description":"","content":"Vietnam Cloud Day 2025 ‚Äî Ho Chi Minh City Connect Edition (Builders)\nDate: 18 September 2025 | 09:00 - 17:00 (VNT)\nLocation: Amazon Web Services Vietnam (Ho Chi Minh City)\nPersonal Reflection ‚Äî Vietnam Cloud Day (HCMC Connect) Attending Vietnam Cloud Day gave me a broad and practical view of how AWS is helping developers and builders adopt cloud technologies in Vietnam. The event combined high‚Äëlevel plenary sessions with focused in‚Äëperson breakout tracks on Generative AI, Data Analytics, Migration \u0026amp; Modernization, and Security. I walked away with both strategic context and concrete technical ideas to try.\nKey takeaways I gained:\nGenerative AI \u0026amp; Roadmap: Sessions on GenAI covered not only foundation models but also the end‚Äëto‚Äëend roadmap for adoption ‚Äî building a data foundation, selecting models, and operationalizing GenAI with considerations for cost, latency, and governance. Data Foundation for AI: A reliable data platform (ingest, storage, processing, governance) is essential to scale AI and analytics workloads. The talks highlighted AWS services and patterns useful for building such a foundation. Migration \u0026amp; Modernization: Case studies about large migrations (including VMware modernizations) emphasized planning for minimal downtime, clear landing zones, and a phased path to modernize and optimize workloads post‚Äëmigration. AI‚ÄëDriven Development Lifecycle (AI‚ÄëDLC): The idea of embedding AI into the software development lifecycle (automating code generation, testing, and reviews) promises productivity gains but requires governance to maintain code quality and security. Securing Generative AI: Security for GenAI needs a multi‚Äëlayered approach ‚Äî infrastructure, model, and application layers must be protected via encryption, zero‚Äëtrust access, and monitoring to mitigate data exposure risks. AI Agents \u0026amp; Automation: Presentations about AI agents opened my eyes to how agents can automate complex workflows; however, they also highlighted the need for oversight and safeguards. What impressed me most:\nThe combination of strategic keynotes (from senior leaders) and hands‚Äëon technical breakout sessions provided both vision and practical steps. Local case studies (e.g., Techcombank, OCB) made the lessons directly relevant to Vietnamese businesses. The GenAI track offered a clear list of services and patterns to prototype quickly, which I can apply to internal POCs. Immediate actions I plan to take:\nBuild a short POC for a GenAI data pipeline (ingest ‚Üí index ‚Üí RAG) to validate feasibility and cost for a small internal dataset. Prototype an AI‚ÄëDLC task (e.g., auto‚Äëgenerated unit tests) to evaluate productivity improvements and governance needs. Perform a security checklist for any GenAI prototype: identify sensitive data, enforce encryption and access controls, and define guardrails before wider testing. Conclusion:\nVietnam Cloud Day was a highly productive event that gave me actionable ideas for GenAI experiments, data platform improvements, and secure modernization strategies. I can convert these notes into a one‚Äëpage summary or a short slide deck if required for the report.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives (IAM Roles on EC2) This week focuses on authorizing applications to access AWS services securely by using IAM roles for EC2 rather than long-lived access keys.\nUnderstand IAM basics and why access keys for application access are discouraged. Configure IAM roles with proper policies and trust relationships for EC2 instances. Demonstrate how applications use instance profile credentials to access AWS APIs without embedding secrets. Practice creating and deleting keys, testing access via AWS CLI/SDK, and cleaning resources. Tasks to be carried out this week (aligned with https://000048.awsstudygroup.com/) Day Task Start Date Completion Date Reference Material 1 Module 1 ‚Äî Prepare: Review IAM basics, instance profiles, and prerequisites. Ensure you have AWS CLI configured and an EC2 Key Pair if needed. 09/30/2025 09/30/2025 https://000048.awsstudygroup.com/1-prepare/ 2 Module 2 ‚Äî Use access key: Generate a temporary Access Key/Secret and test how apps use keys to access services. Learn the drawbacks and risks of long-lived credentials. 09/31/2025 09/31/2025 https://000048.awsstudygroup.com/2-accesskey/ 3 Module 2 ‚Äî Secure key management: Rotate and revoke keys; verify access control; confirm apps fail when keys are removed. Understand why this is not ideal for production. 10/01/2025 10/01/2025 https://000048.awsstudygroup.com/2-accesskey/ 4 Module 3 ‚Äî IAM Role for EC2: Create IAM role and policy granting least privilege (e.g., S3 read). Set trust relationship and attach to EC2. 13/02/2025 13/02/2025 https://000048.awsstudygroup.com/3-iamroleec2/ 5 Module 3 ‚Äî Instance Profile Validation: Launch EC2 with the IAM role, verify the application or CLI can access services without Access Keys, and confirm credentials rotate automatically. 10/03/2025 10/03/2025 https://000048.awsstudygroup.com/3-iamroleec2/ 6 Module 3 ‚Äî Application changes: Update app configuration to use role-based API access; test S3 or DynamoDB access from app without local credentials. 10/04/2025 10/04/2025 https://000048.awsstudygroup.com/3-iamroleec2/ 7 Module 4 ‚Äî Cleanup and Documentation: Remove test Access Keys, detach IAM role if needed, terminate instances, and document steps and security lessons learned. 10/05/2025 10/05/2025 https://000048.awsstudygroup.com/4-cleanup/ Week 4 Achievements (aligned with https://000048.awsstudygroup.com/): Completed the IAM-based workflow for EC2: prepared prerequisites and validated the difference between Access Keys and IAM Roles for instances. Generated temporary Access Keys for testing, recorded the risk and steps to rotate/revoke them, and confirmed applications can be locked down. Created an IAM role with least-privilege policy, attached it to an instance profile, and verified role-based API access from the EC2 instance. Updated application configuration to use instance profile credentials and validated access to AWS services (e.g., S3) without embedding credentials. Completed resource cleanup by removing test keys, detaching roles if needed, terminating instances, and capturing a short summary of lessons learned. Quick Reference / Commands # Create a user (if needed for test cases) and create access key (DO NOT use in production): aws iam create-user --user-name test-user aws iam create-access-key --user-name test-user # Create an IAM role with trust policy for EC2 aws iam create-role --role-name EC2RoleForApp --assume-role-policy-document file://ec2-trust-policy.json # Attach policy to role (example: S3 read access) aws iam attach-role-policy --role-name EC2RoleForApp --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess # Attach role to instance profile and verify aws iam create-instance-profile --instance-profile-name EC2RoleForAppProfile aws iam add-role-to-instance-profile --instance-profile-name EC2RoleForAppProfile --role-name EC2RoleForApp # Run instance with the instance profile (example CLI) aws ec2 run-instances --image-id ami-xxxxxxxx --count 1 --instance-type t3.micro --iam-instance-profile Name=EC2RoleForAppProfile --key-name my-key --security-group-ids sg-xxxx --subnet-id subnet-xxxx # Revoke/Delete access key aws iam delete-access-key --user-name test-user --access-key-id AKIAxxxxxxxxx References IAM Role on EC2: https://000048.awsstudygroup.com/3-iamroleec2/ Use Access Key (risk \u0026amp; rotation): https://000048.awsstudygroup.com/2-accesskey/ Preparation \u0026amp; Cleanup: https://000048.awsstudygroup.com/1-prepare/ and https://000048.awsstudygroup.com/4-cleanup/ "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-systemvalidation/","title":"System Validation","tags":[],"description":"","content":"Testing Strategy We have successfully deployed the backend and seeded the admin user. Now, we need to validate the system functionality.\nWe will simulate a Client Application (like a React Native app) making requests to your Serverless Backend.\nTools: You can use Postman, Thunder Client (VS Code Extension), or simply cURL in your terminal.\nContent Retrieve Configuration Test Admin Login Test Create Listing Test Public Access Verify Resources "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-systemvalidation/5.5.5-verify-resources/","title":"Verify Resources","tags":[],"description":"","content":"Verify Resources in Console If all requests succeed, let\u0026rsquo;s verify the data persistence in AWS Console.\nThis confirms that data is actually being stored in DynamoDB, not just returned from Lambda\u0026rsquo;s memory.\nStep 1: Navigate to DynamoDB Console Go to DynamoDB Console Select Tables from the left sidebar You should see all the tables created by your CDK stack:\nBoardingHouseListings UserProfiles OTPVerifications UserFavorites SupportRequests SearchHistory UserPreferences Step 2: Explore the Listings Table Click on the BoardingHouseListings table Click Explore table items button You should see the listing you created in Test Case 2.\nStep 3: Verify the Data Check that the record contains:\nlistingId: Auto-generated unique identifier title: \u0026ldquo;Luxury Apartment in District 1\u0026rdquo; description: \u0026ldquo;Full furniture, near manufacturing hub\u0026rdquo; price: 5000000 address: \u0026ldquo;123 Le Loi, District 1, HCMC\u0026rdquo; area: 45 amenities: Array containing [\u0026ldquo;Wifi\u0026rdquo;, \u0026ldquo;AC\u0026rdquo;, \u0026ldquo;Parking\u0026rdquo;] createdAt: Timestamp of creation landlordId: User ID of the admin who created it Step 4: Verify User Profile Go back to Tables Click on UserProfiles table Click Explore table items You should see the admin user profile created in Section 5.4:\nuserId: Cognito user ID email: Your admin email fullName: Admin\u0026rsquo;s full name userType: \u0026ldquo;admin\u0026rdquo; createdAt: Timestamp Verification Checklist Confirm the following:\n‚úÖ API Gateway is routing requests correctly ‚úÖ Lambda is processing requests and executing business logic ‚úÖ Cognito is authenticating users successfully ‚úÖ DynamoDB is storing and retrieving data correctly ‚úÖ Protected routes require authentication ‚úÖ Public routes are accessible without authentication ‚úÖ IAM permissions allow Lambda to access all necessary services System Architecture Validation You have successfully validated a complete serverless architecture:\nClient (Postman/cURL) ‚Üì API Gateway (REST API) ‚Üì Lambda Function (Node.js) ‚Üì ‚îú‚îÄ‚Üí Cognito (Authentication) ‚îú‚îÄ‚Üí DynamoDB (Data Storage) ‚îú‚îÄ‚Üí SNS (Notifications - not tested) ‚îú‚îÄ‚Üí Bedrock (AI - not tested) ‚îî‚îÄ‚Üí Location Service (Maps - not tested) Congratulations! Your serverless backend is fully functional and ready for frontend integration.\nNext Steps Test additional endpoints (user registration, favorites, search, etc.) Monitor Lambda execution in CloudWatch Logs Set up CloudWatch alarms for errors Integrate with a frontend application Implement additional features (AI chat, map integration, notifications) Keep your API URL and admin credentials secure. Consider using environment variables or secrets management in production.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Serverless Backend with AWS CDK Overview In this workshop, you will build a complete Serverless Backend for the FindNest application - a boarding house rental platform. You will use AWS CDK (Cloud Development Kit) to define and deploy your infrastructure as code.\nYou will learn how to:\nDeploy serverless infrastructure using AWS CDK with TypeScript Build RESTful APIs with API Gateway and Lambda Implement authentication with Amazon Cognito Store data in DynamoDB with proper data modeling Integrate AI capabilities with Amazon Bedrock (Claude 3) Add map functionality with Amazon Location Service Apply security best practices with IAM policies Manage infrastructure lifecycle (deploy and cleanup) Architecture You will deploy a modern serverless architecture consisting of:\nAPI Gateway - RESTful API endpoints AWS Lambda - Serverless compute (Node.js) Amazon DynamoDB - NoSQL database (7 tables) Amazon Cognito - User authentication and authorization Amazon S3 - Image storage Amazon Location Service - Maps and geocoding Amazon Bedrock - AI/ML with Claude 3 Amazon SNS - SMS notifications CloudWatch Logs - Monitoring and logging Content Workshop Overview Prerequisites Deploy Backend Seeding Data System Validation Clean Up Resources "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives (AWS Cloud9 \u0026amp; Amazon S3) This week covers using AWS Cloud9 as a cloud IDE for development and how to host static websites using Amazon S3. You will learn to create Cloud9 environments, use the built-in terminal/CLI, and deploy static content to S3 with best practices for public access and acceleration (CloudFront).\nCreate and use AWS Cloud9 environments for development and AWS CLI operations. Host a static website on Amazon S3: enable static hosting, configure public objects, and verify public access. Accelerate and secure static hosting using Amazon CloudFront and S3 best practices (versioning, replication, and public access block). Clean up resources and apply cost/cultural best practices. Tasks to be carried out this week (aligned with https://000049.awsstudygroup.com/ and https://000057.awsstudygroup.com/): Day Task Start Date Completion Date Reference Material 1 Module 1 (Cloud9) ‚Äî Create Cloud9 environment: Create an AWS Cloud9 instance and explore the IDE, settings, and preferences. 10/13/2025 10/13/2025 https://000049.awsstudygroup.com/1-createcloud9/ 2 Module 2 (Cloud9) ‚Äî Cloud9 basic operations: Use the terminal, code editor, debugging, and the AWS CLI inside Cloud9 to manage resources. 10/14/2025 10/14/2025 https://000049.awsstudygroup.com/2-basicfeature/ 3 Module 3 (Cloud9) ‚Äî Using AWS CLI: Practice using AWS CLI from Cloud9 (deploy, navigate, troubleshoot), and set up IAM roles/credentials as needed for the environment. 10/15/2025 10/15/2025 https://000049.awsstudygroup.com/3-useawscli/ 4 Module 1 (S3) ‚Äî Prepare S3 and enable static website hosting: Create a bucket, enable static hosting, and set index/error documents. 10/16/2025 10/16/2025 https://000057.awsstudygroup.com/1-introduce/ 5 Module 3 (S3) ‚Äî Configure public access and objects: Configure the public access block and set object ACLs or bucket policies to allow public sites, and upload a sample static site (HTML/CSS/JS). 10/17/2025 10/17/2025 https://000057.awsstudygroup.com/3-staticwebsite/ 6 Module 7 \u0026amp; 8 (S3) ‚Äî Acceleration \u0026amp; Versioning: Setup CloudFront to accelerate the S3 site; enable bucket versioning and consider lifecycle/replication options if required. 10/18/2025 10/18/2025 https://000057.awsstudygroup.com/7-cloudfront/ 7 Module 11 ‚Äî Test \u0026amp; Cleanup: Verify the website is accessible, automate uploads via Cloud9/CLI, and cleanup resources (Cloud9 environment, S3 buckets, CloudFront distributions). 10/19/2025 10/19/2025 https://000057.awsstudygroup.com/6-testwebsite/ Week 5 Achievements (aligned with Cloud9 \u0026amp; S3 workshop content): Created and explored an AWS Cloud9 environment; used the built-in editor and terminal for development and AWS CLI operations. Performed Cloud9 tasks: configured environment settings, used the debugger, and ran sample scripts from the Cloud9 terminal. Created an S3 bucket and enabled static website hosting; uploaded sample web content and verified website availability. Configured public access (bucket policy or ACL), tested object-level access, and set up CloudFront distribution for performance. Implemented versioning for the S3 bucket and reviewed cleanup steps to delete Cloud9 environments, buckets, and CloudFront resources to avoid charges. Automated uploads using AWS CLI from Cloud9, demonstrating an integrated development and deployment workflow. Quick Reference / Commands (Cloud9 \u0026amp; S3) # Cloud9: Create an environment in Console or via CloudFormation/CLI if supported # Using AWS CLI inside Cloud9 to upload site aws s3 mb s3://my-static-site-bucket --region us-east-1 aws s3 website s3://my-static-site-bucket --index-document index.html --error-document error.html aws s3 cp ./site s3://my-static-site-bucket/ --recursive --acl public-read # CloudFront distribution creation snippet (example) aws cloudfront create-distribution --origin-domain-name my-static-site-bucket.s3.amazonaws.com # Enable versioning aws s3api put-bucket-versioning --bucket my-static-site-bucket --versioning-configuration Status=Enabled # Cleanup example aws s3 rb s3://my-static-site-bucket --force References: AWS Cloud9: https://000049.awsstudygroup.com/ Static Website Hosting (S3): https://000057.awsstudygroup.com/ "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Why is cleanup important? In a Cloud environment, you pay for what you provision. Even though Serverless services like Lambda and DynamoDB (On-Demand) have a generous Free Tier or low idle costs, other resources might incur charges over time:\nS3 Storage: You pay for the data stored in buckets Amazon Location Service: Storing Place Indexes or using Maps CloudWatch Logs: Stored log data To prevent unexpected billing, always clean up your environment after completing a workshop.\nContent Destroy the Stack Verify Resource Deletion Workshop Conclusion "},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Amazon Web Services - AWS] from [09-06-2025] to [12-09-2025], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [Findnest], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo reflect on my internship in a constructive and actionable way, I reorganised my self-evaluation into clear sections: objectives, key accomplishments, skills developed, concrete examples, challenges faced and how I addressed them, feedback received, and development goals going forward.\nInternship context\nRole: [Backend + Security] Team/Project: [Find Nest] Period: [15-09-2025] ‚Äì [07-12-2025] Objectives I set at the start\nLearn and apply cloud fundamentals (AWS networking, S3, VPC endpoints). Implement and document a secure hybrid access lab for S3 using VPC endpoints. Improve scripting and automation skills (CloudFormation, bash, AWS CLI). Enhance teamwork, reporting, and presentation abilities. Key achievements\nDesigned and authored a full end‚Äëto‚Äëend workshop: \u0026ldquo;Secure Hybrid Access to S3 using VPC Endpoints\u0026rdquo; including architecture diagrams, step‚Äëby‚Äëstep labs, and cleanup procedures. Implemented the CloudFormation stacks used in the workshop and validated deployment in the us-east-1 region (reduced manual setup time for participants by ~15 minutes). Created reproducible test procedures for Gateway and Interface endpoints, and produced troubleshooting notes for common DNS and VPN issues. Wrote clear documentation and translated core workshop materials into Vietnamese and English to support a broader audience. Skills developed\nTechnical: AWS VPC, Route 53 Resolver, Site‚Äëto‚ÄëSite VPN simulation (strongSwan), S3 endpoints (gateway \u0026amp; interface), CloudFormation, AWS CLI, Session Manager. Software: Shell scripting (fallocate, aws cli automation), Markdown documentation and diagrams. Soft skills: Technical writing, knowledge transfer (workshop delivery), teamwork and cross‚Äëfunctional collaboration. Concrete examples / Evidence\nWorkshop content: content/5-Workshop/* (architecture, lab steps, cleanup). Automation: CloudFormation template links included in the lab (PLCloudSetup, PLOnpremSetup). Tests performed: Upload/download operations via Session Manager on EC2 instances using both Gateway and Interface endpoints. Challenges and mitigation\nDNS resolution for PrivateLink endpoints was initially inconsistent across simulated on‚Äëprem DNS; I implemented a Route 53 resolver forwarding rule and documented the exact steps to reproduce and fix the issue. Large file uploads (1GB test files) occasionally hit timeouts; I added guidance to use smaller sample files when bandwidth is constrained and included tips to monitor and retry uploads. Feedback received \u0026amp; actions taken\nFeedback: Request to make labs more beginner friendly and to provide shorter quick‚Äëstart exercises. Action: Added a quick start \u0026ldquo;smoke test\u0026rdquo; that performs minimal checks and a separate deep‚Äëdive section for advanced troubleshooting. Development areas (plan)\nDiscipline \u0026amp; time management: adopt a weekly task plan with prioritized tickets and short daily notes; review progress with mentor. Problem solving: practice structured troubleshooting (hypothesis‚Üítest‚Üíisolate) and log findings to a shared knowledge base. Communication: present one lab walkthrough to the team and request focused feedback on clarity and pacing. Goals for the next 6 months\nContribute to at least two more workshop modules and automate one end‚Äëto‚Äëend test with CI (e.g., a basic CloudFormation validation run). Complete an AWS certification or internal training module related to networking or security. Final reflection This internship provided a strong bridge between theoretical study and practical engineering. I improved both technical capabilities (networking, AWS services, automation) and professional skills (documentation, collaboration, feedback handling). I will continue to iterate on the materials I created and use the feedback I received to make the next workshops clearer and more resilient for participants.\nQuick self-assessment summary (snapshot) Area Rating (1‚Äì5) Notes Technical knowledge 4 Solid understanding of VPC endpoints and Route 53 resolver patterns Ability to learn 4 Rapidly picked up new AWS services and troubleshooting methods Proactiveness 5 Took initiative to create workshop materials and automate steps Responsibility \u0026amp; reliability 4 Delivered deliverables on time, with incremental improvements Communication 3 Improved but needs practice in concise presentations Problem solving 3 Good at identifying causes; will improve speed and coverage "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives (Amazon RDS) This week covers Amazon Relational Database Service (RDS): creating managed database instances, selecting the appropriate engine and storage, setting up availability, backups and security, and deploying an application that uses RDS. You\u0026rsquo;ll learn to perform hands-on RDS tasks, operate backups/restores, and understand scaling, monitoring, and cleanup.\nUnderstand RDS capabilities and supported engines (Aurora, MySQL, MariaDB, SQL Server, PostgreSQL, Oracle). Configure storage options (gp3, io2) and high-availability (Multi-AZ, Read Replicas). Launch RDS instances, configure database and security settings, and connect from EC2. Deploy a sample application that uses RDS and practice backups and restore operations. Tasks to be carried out this week (aligned with https://000005.awsstudygroup.com/): Day Task Start Date Completion Date Reference Material 1 Module 1 ‚Äî Introduction \u0026amp; Preparation: Read the RDS overview, supported engines, and identify the DB engine to use. Prepare VPC, subnet, and Security Group rules for DB access. 10/20/2025 10/20/2025 https://000005.awsstudygroup.com/1-introduce/ 2 Module 2 ‚Äî Prepare prerequisites: Launch or prepare EC2 instances as application hosts (if required), configure IAM roles and create RDS subnet groups and parameter groups. 10/21/2025 10/22/2025 https://000005.awsstudygroup.com/2-prerequiste/ 3 Module 3 ‚Äî Create EC2 instance(s): Launch an EC2 instance to connect and test DB access (if your app is deployed on EC2). Configure networking and security for connectivity. 10/22/2025 10/22/2025 https://000005.awsstudygroup.com/3-create-ec2/ 4 Module 4 ‚Äî Create RDS database instance: Create RDS instance (engine choice), storage type, Multi-AZ or Read Replica options, and networking settings; validate connectivity from EC2. 10/23/2025 10/23/2025 https://000005.awsstudygroup.com/4-create-rds/ 5 Module 5 ‚Äî Application Deployment: Deploy sample app (e.g., user management or web app) to EC2 and configure it to connect to the RDS instance (DB endpoint, credentials). 10/24/2025 10/24/2025 https://000005.awsstudygroup.com/5-deploy-app/ 6 Module 6 ‚Äî Backup and Restore: Practice automated backups, create manual snapshots, test point-in-time recovery and manual snapshot restore into new instances for validation. 10/25/2025 10/25/2025 https://000005.awsstudygroup.com/6-backup/ 7 Module 7 ‚Äî Clean up and documentation: Terminate instances, remove DB instances appropriately (snapshot if necessary), and document steps and observations. 10/26/2025 10/26/2025 https://000005.awsstudygroup.com/7-cleanup/ Week 6 Achievements (aligned with https://000005.awsstudygroup.com/): Analysed and selected an appropriate DB engine for the sample application (e.g., MySQL/PostgreSQL/Aurora) and selected storage type (gp3/io2) based on performance needs. Created subnet groups and parameter groups to support the RDS deployment and configured secure network access using Security Groups and VPC settings. Launched RDS instances and validated application connectivity from EC2; observed typical DB behavior and identified best-ops metrics. Implemented automated backups and created manual snapshots; successfully tested restore and recovery procedures into a new DB instance. Deployed a sample app that uses RDS for data persistence and validated basic CRUD operations. Completed resource cleanup and documented all steps, configuration choices, and lessons learned for reproducibility and cost control. Quick Reference / Commands (RDS \u0026amp; Example) # Create a DB Subnet Group aws rds create-db-subnet-group --db-subnet-group-name my-subnet-group --db-subnet-group-description \u0026#34;week6-subnet-group\u0026#34; --subnet-ids subnet-xxxx subnet-yyyy # Create a MySQL RDS instance (example) aws rds create-db-instance --db-instance-identifier mydbinstance --db-instance-class db.t3.micro --engine mysql --master-username admin --master-user-password \u0026#34;ReplaceWithSecurePass!\u0026#34; --allocated-storage 20 --db-subnet-group-name my-subnet-group --vpc-security-group-ids sg-xxxx # Create a snapshot aws rds create-db-snapshot --db-instance-identifier mydbinstance --db-snapshot-identifier mydbsnapshot # Restore from snapshot (into a new instance) aws rds restore-db-instance-from-db-snapshot --db-instance-identifier mydb-restored --db-snapshot-identifier mydbsnapshot # Delete DB instance (finalize cleanup) aws rds delete-db-instance --db-instance-identifier mydbinstance --skip-final-snapshot References: Amazon RDS overview \u0026amp; modules: https://000005.awsstudygroup.com/ "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Overview Week 7 focuses on Amazon Lightsail ‚Äî a simplified platform for deploying small web applications and managed databases. This week mirrors the \u0026ldquo;AMAZON LIGHTSAIL WORKSHOP - COST OPTIMIZATION ON AWS\u0026rdquo; (https://000045.awsstudygroup.com/).\nObjectives Get familiar with Amazon Lightsail and its available blueprints. Deploy open-source applications: WordPress, PrestaShop and Akaunting on Lightsail. Create and manage Lightsail relational databases, snapshots, and backups. Apply basic application security and cost-optimization best practices. Practice resource cleanup to avoid unintended charges. Prerequisites An AWS account with permissions to create Lightsail resources (Free Tier may apply). Optional: configured awscli, or access to the AWS Management Console. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Workshop overview \u0026amp; environment preparation: read the guide, check permissions, and set learning goals. 10/27/2025 10/27/2025 https://000045.awsstudygroup.com/ 2 Module 1 ‚Äî Deploy a Database on Lightsail: create a relational database instance, verify connectivity, and configure backups. 10/28/2025 10/28/2025 https://000045.awsstudygroup.com/1-database/ 3 Module 2 ‚Äî Deploying a WordPress Instance: use a Lightsail blueprint or OS-based setup to run WordPress and verify the site. 10/29/2025 10/29/2025 https://000045.awsstudygroup.com/2-wp-instance/ 4 Module 3 ‚Äî Deploying Prestashop E-Commerce Instance: deploy Prestashop using the Application Blueprint and test workflows. 10/30/2025 10/30/2025 https://000045.awsstudygroup.com/3-e-commerce-instance/ 5 Module 4 ‚Äî Deploying Akaunting Instance: deploy Akaunting, ensure DB connectivity and basic configuration. 10/31/2025 10/31/2025 https://000045.awsstudygroup.com/4-akaunting-instance/ 6 Module 5‚Äì7 ‚Äî Application Security, Snapshots, Scaling: apply security best practices (firewall rules), create snapshots, and practice scaling/upgrading instances. 11/01/2025 11/01/2025 https://000045.awsstudygroup.com/5-secure-the-applications/, https://000045.awsstudygroup.com/6-create-snapshots/, https://000045.awsstudygroup.com/7-migrate-to-larger-instances/ 7 Module 8‚Äì9 ‚Äî Alarms \u0026amp; Cleanup: create basic alarms, estimate cost impact, and perform resource cleanup following the guide. 11/02/2025 11/02/2025 https://000045.awsstudygroup.com/8-create-alarms/, https://000045.awsstudygroup.com/9-clean-up/ Week 7 Achievements Successfully provision at least one Lightsail relational database and verify connectivity. Run WordPress, PrestaShop or Akaunting on Lightsail (at least one application end-to-end). Create instance snapshots and practice recovery (snapshot ‚Üí new instance). Apply basic security controls (firewall rules, disable unnecessary services). Set up basic alarms for resource monitoring and perform cleanup to avoid charges. Quick Reference (CLI examples) Note: replace --instance-name, --relational-database-name and --blueprint-id with real values.\n# Create a Lightsail instance (example: WordPress blueprint) aws lightsail create-instances --instance-names MyWordpress --availability-zone us-east-1a --blueprint-id wordpress --bundle-id micro_2_0 # Create a relational database aws lightsail create-relational-database --relational-database-name my-db --availability-zone us-east-1a --master-username admin --master-user-password \u0026#39;P@ssw0rd!\u0026#39; # Create an instance snapshot aws lightsail create-instance-snapshot --instance-name MyWordpress --instance-snapshot-name my-wordpress-snap # Get instance information aws lightsail get-instance --instance-name MyWordpress # Delete instance (cleanup) aws lightsail delete-instance --instance-name MyWordpress # Delete relational database (cleanup) aws lightsail delete-relational-database --relational-database-name my-db References Amazon Lightsail workshop: https://000045.awsstudygroup.com/ Workshop modules: Deploy Database, WordPress, PrestaShop, Akaunting, Security, Snapshots, Scaling, Alarms, Cleanup "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Purpose This page collects structured feedback about the First Cloud Journey (FCJ) internship program. The goal is to provide clear, actionable feedback that the FCJ team can use to improve the program and help future interns succeed.\nExecutive summary Overall, the FCJ program provides a highly supportive and practical environment for interns to learn core cloud engineering skills. Mentors and the admin team are responsive, the hands-on labs are relevant to real-world scenarios, and the program covers a broad set of topics. A few targeted improvements would increase intern productivity and reduce time spent on environment setup and repetitive tasks.\nDetailed feedback (categories) Working Environment Rating: 5/5. Supportive team, comfortable workspace, flexible working conditions; consider more regular social or team-building activities. Mentor \u0026amp; Admin Support Rating: 5/5. Mentors provide clear guidance and constructive feedback. Admins handle logistics well. Consider scheduling more structured check-ins and pairing sessions for complex tasks. Learning Relevance Rating: 5/5. Tasks align well with academic knowledge while introducing useful new areas. Continue preserving daily hands-on labs and add small challenge tasks for deeper learning. Skill Development Opportunities Rating: 4/5. Interns learn modern AWS practices and tooling. Company Culture \u0026amp; Team Spirit Rating: 5/5. Positive, collaborative culture. Consider a monthly social event (virtual or in-person) to build stronger peer relationships. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Overview Week 8 covers deploying the FCJ Management application using Amazon EC2 Auto Scaling Groups and an Application Load Balancer. The goal is to build a resilient, scalable architecture that adjusts capacity automatically to meet demand. This worklog follows the workshop at https://000006.awsstudygroup.com/.\nObjectives Understand Auto Scaling Group concepts and how they integrate with Launch Templates and Load Balancers. Create a Launch Template, provision an Application Load Balancer, and attach instances to the ALB. Configure and test an Auto Scaling Group (ASG) with scaling policies. Validate application behavior under load and perform cleanup to avoid charges. Prerequisites AWS account with permissions to create EC2, EC2 Auto Scaling, and Elastic Load Balancing resources. Basic knowledge of EC2 instance creation and AMIs (see Week 3 content for reference). Optional: awscli configured for quick testing from terminal. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Workshop overview \u0026amp; preparation: read the guide and confirm account permissions and quotas. 11/05/2025 11/05/2025 https://000006.awsstudygroup.com/ 2 Module 1 ‚Äî Introduction \u0026amp; Preparation: gather AMI/instance details, security groups, and required roles. 11/06/2025 11/06/2025 https://000006.awsstudygroup.com/1-introduction/, https://000006.awsstudygroup.com/2-preparation/ 3 Module 2 ‚Äî Create Launch Template: create and test a Launch Template (AMI, instance type, user data, key pair, security group). 11/07/2025 11/07/2025 https://000006.awsstudygroup.com/3-create-launch-template/ 4 Module 3 ‚Äî Setup Load Balancer: create an Application Load Balancer, target group, and register targets. 11/08/2025 11/08/2025 https://000006.awsstudygroup.com/4-setup-load-balancer/ 5 Module 4 ‚Äî Create Auto Scaling Group: create ASG using the Launch Template and attach it to the target group; configure min/max/desired capacities. 11/09/2025 11/09/2025 https://000006.awsstudygroup.com/6-create-auto-scaling-group/ 6 Module 5‚Äì6 ‚Äî Test \u0026amp; Test Solutions: simulate load, verify scaling actions, and validate health checks and replacement behavior. 11/11/2025 11/11/2025 https://000006.awsstudygroup.com/5-test/, https://000006.awsstudygroup.com/7-test-solutions/ 7 Module 7 ‚Äî Cleanup: terminate ASG, delete Launch Template, ALB, target groups, and any leftover resources. 11/11/2025 11/11/2025 https://000006.awsstudygroup.com/8-cleanup/ Week 8 Achievements Built a Launch Template and verified instance bootstrapping (user-data) works as expected. Provisioned an Application Load Balancer and registered application instances. Created and validated an Auto Scaling Group with expected scaling behavior under test load. Confirmed health checks and automatic replacement of unhealthy instances. Cleaned up all resources to avoid residual costs. Quick Reference (CLI examples) Note: replace placeholder names with actual values from your account.\n# Create a launch template aws ec2 create-launch-template --launch-template-name my-launch-template --version-description v1 --launch-template-data \u0026#39;{\u0026#34;ImageId\u0026#34;:\u0026#34;ami-0123456789abcdef0\u0026#34;,\u0026#34;InstanceType\u0026#34;:\u0026#34;t3.micro\u0026#34;,\u0026#34;UserData\u0026#34;:\u0026#34;$(echo -n \\\u0026#34;#!/bin/bash\\n...\\\u0026#34; | base64)\u0026#34;}\u0026#39; # Create a target group aws elbv2 create-target-group --name my-targets --protocol HTTP --port 80 --vpc-id vpc-xxxxxxxx # Create an application load balancer aws elbv2 create-load-balancer --name my-alb --subnets subnet-aaa subnet-bbb --security-groups sg-xxxxxx # Register targets (example) aws elbv2 register-targets --target-group-arn arn:aws:elasticloadbalancing:...:targetgroup/my-targets/xxxxxxxx --targets Id=i-0123456789abcdef0 # Create an autoscaling group aws autoscaling create-auto-scaling-group --auto-scaling-group-name my-asg --launch-template LaunchTemplateName=my-launch-template,Version=1 --min-size 1 --max-size 3 --desired-capacity 1 --vpc-zone-identifier \u0026#34;subnet-aaa,subnet-bbb\u0026#34; --target-group-arns arn:aws:elasticloadbalancing:...:targetgroup/my-targets/xxxxxxxx # Delete autoscaling group (cleanup) aws autoscaling delete-auto-scaling-group --auto-scaling-group-name my-asg --force-delete # Delete launch template aws ec2 delete-launch-template --launch-template-name my-launch-template References Deploying FCJ Management Application with Auto Scaling Group: https://000006.awsstudygroup.com/ Modules: Introduction, Preparation, Create Launch Template, Setup Load Balancer, Test, Create Auto Scaling Group, Test Solutions, Cleanup If you\u0026rsquo;d like, I can:\nAdd detailed per-day CLI step-by-step instructions. Generate IaC (CloudFormation/Terraform) snippets for Launch Template/ASG/ALB (note: test locally; do not run in your account without review). Produce a Vietnamese translation of this English content. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Overview Week 9 covers Amazon CloudWatch: metrics, logs, alarms, and dashboards. This week follows the \u0026ldquo;AWS CLOUDWATCH WORKSHOP\u0026rdquo; at https://000008.awsstudygroup.com/ and focuses on collecting observability data, creating alarms, and building dashboards for troubleshooting and cost-conscious monitoring.\nObjectives Understand CloudWatch metrics, logs, alarms, and dashboards. Instrument applications (or simulate metrics) and send custom metrics to CloudWatch. Create alarms and dashboards to monitor system health and trigger actions. Practice cleaning up CloudWatch resources to avoid unnecessary charges. Prerequisites AWS account with permissions to create CloudWatch metrics, logs, alarms, and dashboards. Optional: sample application or scripts that emit metrics/logs (can use awscli to push test metrics). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Workshop overview \u0026amp; preparation: review CloudWatch concepts and confirm IAM permissions. 11/12/2025 11/12/2025 https://000008.awsstudygroup.com/ 2 Preparatory steps: create or identify an instance/app that emits metrics/logs; create a log group if needed. 11/13/2025 11/13/2025 https://000008.awsstudygroup.com/2-preparatory-steps/ 3 CloudWatch Metrics: view built-in metrics, push custom metrics (using put-metric-data), and explore namespaces. 11/14/2025 11/14/2025 https://000008.awsstudygroup.com/3-cloud-watch-metric/ 4 CloudWatch Logs: create log groups/streams, push test logs, and query logs with CloudWatch Logs Insights. 11/15/2025 11/15/2025 https://000008.awsstudygroup.com/4-cloud-watch-logs/ 5 CloudWatch Alarms: create alarms for important metrics and test alarm states (OK ‚Üí ALARM ‚Üí INSUFFICIENT_DATA). 11/16/2025 11/16/2025 https://000008.awsstudygroup.com/5-cloud-watch-alarm/ 6 CloudWatch Dashboards: build a dashboard that aggregates key metrics and logs for the application. 11/17/2025 11/17/2025 https://000008.awsstudygroup.com/6-cloud-watch-dashboard/ 7 Cleanup: delete test metrics, log groups, alarms, and dashboards created during the exercises. 11/18/2025 11/18/2025 https://000008.awsstudygroup.com/7-clean-up-resources/ Week 9 Achievements Viewed and analyzed built-in CloudWatch metrics and pushed custom metrics for application testing. Created log groups and used CloudWatch Logs Insights to run queries and identify issues. Defined alarms for critical metrics and validated alert state transitions. Assembled a CloudWatch dashboard that surfaces application health and performance indicators. Cleaned up all CloudWatch test artifacts. Quick Reference (CLI examples) Note: replace resource names and regions with your own values.\n# Put a custom metric to CloudWatch aws cloudwatch put-metric-data --metric-name PageViews --namespace \u0026#34;FCJ/Application\u0026#34; --value 1 --unit Count --region us-east-1 # Create a log group aws logs create-log-group --log-group-name \u0026#34;/fcj/app/logs\u0026#34; --region us-east-1 # Put a log event (requires sequence token for persistent streams; good for testing) aws logs put-log-events --log-group-name \u0026#34;/fcj/app/logs\u0026#34; --log-stream-name \u0026#34;test-stream\u0026#34; --log-events timestamp=$(date +%s)000,message=\u0026#34;Test log entry\u0026#34; --region us-east-1 # Create a CloudWatch alarm aws cloudwatch put-metric-alarm --alarm-name HighCPUAlarm --metric-name CPUUtilization --namespace AWS/EC2 --statistic Average --period 60 --threshold 80 --comparison-operator GreaterThanThreshold --evaluation-periods 2 --alarm-actions arn:aws:sns:us-east-1:123456789012:MyTopic --dimensions Name=InstanceId,Value=i-0123456789abcdef0 --region us-east-1 # Create a dashboard from a JSON body file aws cloudwatch put-dashboard --dashboard-name FCJ-App-Dashboard --dashboard-body file://dashboard.json --region us-east-1 # List alarms aws cloudwatch describe-alarms --region us-east-1 # Delete a dashboard aws cloudwatch delete-dashboards --dashboard-names FCJ-App-Dashboard --region us-east-1 # Delete a log group aws logs delete-log-group --log-group-name \u0026#34;/fcj/app/logs\u0026#34; --region us-east-1 References AWS CloudWatch workshop: https://000008.awsstudygroup.com/ Modules: Introduction, Preparatory steps, CloudWatch Metrics, CloudWatch Logs, CloudWatch Alarms, CloudWatch Dashboards, Cleanup resources If you\u0026rsquo;d like, I can:\nAdd detailed per-day CLI step-by-step instructions and example dashboard JSON. Provide CloudWatch Logs Insights example queries for common troubleshooting scenarios. Produce a Vietnamese translation of this page. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Overview Week 10 covers two complementary workshops:\n\u0026ldquo;SET UP HYBRID DNS WITH ROUTE 53 RESOLVER\u0026rdquo; (https://000010.awsstudygroup.com/) ‚Äî design and implement a hybrid DNS architecture using Route¬†53 Resolver inbound/outbound endpoints and rules. \u0026ldquo;GETTING STARTED WITH THE AWS CLI\u0026rdquo; (https://000011.awsstudygroup.com/) ‚Äî install, configure, and use the AWS CLI across common services. Combining both topics gives practical skills: manage infrastructure via the CLI and integrate on-prem DNS with AWS for hybrid scenarios.\nObjectives Install and configure the AWS CLI and basic profiles. Use the CLI to inspect and manage AWS resources (S3, EC2, IAM, VPC, Route¬†53). Deploy Route¬†53 Resolver inbound/outbound endpoints and Resolver rules to enable DNS forwarding between AWS and on-prem systems. Validate DNS resolution flow and clean up resources after testing. Prerequisites AWS account with permissions for Route¬†53 Resolver, EC2, VPC, IAM, and basic services used in CLI examples. Local environment with AWS CLI installed (or use CloudShell). If testing the hybrid DNS flow, an on‚Äëprem or jump host that can reach the Resolver endpoints (RDGW/remote host). See the workshop for network topology details. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Workshop overview \u0026amp; environment preparation: read both guides, confirm IAM permissions, and set CLI profile(s). 11/19/2025 11/19/2025 https://000010.awsstudygroup.com/, https://000011.awsstudygroup.com/ 2 AWS CLI: install/upgrade AWS CLI v2, configure default and named profiles, practice aws configure, aws sts get-caller-identity. 11/20/2025 11/20/2025 https://000011.awsstudygroup.com/3-installcli/, https://000011.awsstudygroup.com/1-introduce/ 3 AWS CLI hands-on: explore S3, SNS, IAM, VPC and EC2 via CLI (aws s3 ls, aws ec2 describe-instances, aws iam list-roles). 11/21/2025 11/21/2025 https://000011.awsstudygroup.com/4-infras/, https://000011.awsstudygroup.com/5-s3/ 4 Route¬†53 Resolver: prepare network (VPC/subnets), create inbound/outbound endpoints, and configure Resolver rules. 11/22/2025 11/22/2025 https://000010.awsstudygroup.com/2-prerequiste/, https://000010.awsstudygroup.com/5-setuphyriddns/ 5 Connect to RDGW / test DNS: verify on‚Äëprem to AWS resolution and AWS to on‚Äëprem forwarding; troubleshoot using dig/nslookup. 11/23/2025 11/23/2025 https://000010.awsstudygroup.com/3-connecttordgw/, https://000010.awsstudygroup.com/5-setuphyriddns/ 6 Validate and automate: use AWS CLI to list Resolver endpoints/rules, and test scripted flows; capture test results. 11/24/2025 11/24/2025 https://000010.awsstudygroup.com/ , https://000011.awsstudygroup.com/ 7 Cleanup: remove Resolver endpoints and rules, remove any test resources, and ensure IAM keys/profiles are rotated or deleted if temporary. 11/25/2025 11/25/2025 https://000010.awsstudygroup.com/6-cleanup/, https://000011.awsstudygroup.com/11-cleanup/ Week 10 Achievements Installed and configured AWS CLI profiles and validated access with aws sts get-caller-identity. Used CLI to inspect S3, EC2, IAM, VPC resources and automate small tasks. Deployed Route¬†53 Resolver inbound/outbound endpoints and resolver rules to forward DNS between on‚Äëprem and AWS. Validated DNS queries flow between on‚Äëprem and AWS and documented troubleshooting steps. Performed cleanup to remove test resources and minimize charges. Quick Reference (CLI examples) Replace placeholders with values from your account and region.\n# Configure AWS CLI (interactive) aws configure --profile fcj # Verify identity aws sts get-caller-identity --profile fcj # List S3 buckets aws s3 ls --profile fcj # Describe EC2 instances aws ec2 describe-instances --profile fcj # Create a Route 53 Resolver outbound endpoint (example) aws route53resolver create-resolver-endpoint --creator-request-id \u0026#34;fcj-outbound-1\u0026#34; --name \u0026#34;fcj-outbound\u0026#34; --direction OUTBOUND --ip-addresses SubnetId=subnet-aaa,Ip=10.0.1.10 SubnetId=subnet-bbb,Ip=10.0.2.10 --security-group-ids sg-xxxx --profile fcj # Create a Route 53 Resolver rule to forward specific domain to on-prem DNS aws route53resolver create-resolver-rule --creator-request-id \u0026#34;fcj-rule-1\u0026#34; --name \u0026#34;forward-onprem\u0026#34; --rule-type FORWARD --domain-name \u0026#34;corp.example.local\u0026#34; --target-ips Ip=10.0.100.10,Port=53 --profile fcj # Associate resolver rule with a VPC aws route53resolver associate-resolver-rule --resolver-rule-id rslvr-rr-xxxxxxxx --vpc-id vpc-xxxxxxxx --profile fcj # List resolver endpoints and rules aws route53resolver list-resolver-endpoints --profile fcj aws route53resolver list-resolver-rules --profile fcj # Cleanup (delete resolver rule and endpoint) aws route53resolver delete-resolver-rule --resolver-rule-id rslvr-rr-xxxxxxxx --profile fcj aws route53resolver delete-resolver-endpoint --resolver-endpoint-id rslvr-out-xxxxxxxx --profile fcj References Set up Hybrid DNS with Route¬†53 Resolver: https://000010.awsstudygroup.com/ Getting Started with the AWS CLI: https://000011.awsstudygroup.com/ If you\u0026rsquo;d like, I can:\nProvide step-by-step CLI commands for each day (safe, non-destructive examples). Generate small IaC snippets (CloudFormation/Terraform) for creating Resolver endpoints (review before running). Produce a Vietnamese translation of this content. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Overview Week 11 combines practical AWS CLI skills with migration planning and hands‚Äëon migration patterns. It draws from:\n\u0026ldquo;GETTING STARTED WITH THE AWS CLI\u0026rdquo; (https://000011.awsstudygroup.com/) ‚Äî to operate and automate AWS resources via the command line. \u0026ldquo;MIGRATE TO AWS\u0026rdquo; (https://cloudjourney.awsstudygroup.com/2-migrate/) ‚Äî to understand migration strategies, assessment, and migration tools (VM import/export, DMS, etc.). This week is about using the CLI to help assess, orchestrate, and validate migrations to AWS.\nObjectives Install and master basic AWS CLI workflows and profiles for automation. Learn migration planning concepts: assessment, lift-and-shift, replatforming, database migration (DMS), and validation. Use the CLI to collect inventory, run simple migration operations (VM import/export or DMS where applicable), and validate migrated resources. Build a checklist for safe migration and cleanup steps to avoid drift and excess cost. Prerequisites AWS account with permissions for EC2, S3, IAM, DMS (if available), and related services. Local environment with AWS CLI installed and configured profiles for the target account. Sample workload or VM artifacts for migration exercises (optional ‚Äî use simulated artifacts if production data is not available). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Workshop overview \u0026amp; preparation: review both guides, confirm IAM permissions, set up CLI profiles, and identify workloads to assess. 11/26/2025 11/26/2025 https://000011.awsstudygroup.com/, https://cloudjourney.awsstudygroup.com/2-migrate/ 2 CLI fundamentals: install/verify AWS CLI v2, create named profiles, practice common commands (sts, s3, ec2, iam). 11/27/2025 11/27/2025 https://000011.awsstudygroup.com/3-installcli/, https://000011.awsstudygroup.com/1-introduce/ 3 Inventory \u0026amp; assessment: use CLI to list instances, gather metadata, export inventories (CSV/JSON) for migration planning. 11/28/2025 11/28/2025 https://000011.awsstudygroup.com/4-infras/, https://cloudjourney.awsstudygroup.com/2-migrate/ 4 VM Import / Export \u0026amp; Storage: review VM import/export workflows or use S3 to stage artifacts; practice small import/export (dry-run). 11/29/2025 11/29/2025 https://cloudjourney.awsstudygroup.com/, https://000014.awsstudygroup.com/ 5 Database migration planning: overview DMS and SCT patterns; prepare source/target endpoints and assess schema conversion needs. 11/30/2025 11/30/2025 https://000043.awsstudygroup.com/ 6 Migrate \u0026amp; validate: run a small migration demo (e.g., import VM or copy data), validate target resource behavior, and collect logs/metrics. 12/01/2025 12/01/2025 https://cloudjourney.awsstudygroup.com/2-migrate/, https://000011.awsstudygroup.com/ 7 Create migration checklist \u0026amp; cleanup: document steps, rollback procedure, and remove test artifacts (S3, temp roles, DMS tasks). 12/02/2025 12/02/2025 https://cloudjourney.awsstudygroup.com/2-migrate/, https://000011.awsstudygroup.com/11-cleanup/ Week 11 Achievements Configured CLI profiles and automated inventory collection of candidate workloads. Completed a small VM import/export or staged artifact transfer to S3 for migration testing. Reviewed database migration patterns (DMS/SCT) and documented conversion considerations. Validated a migrated resource and produced a migration checklist with rollback/cleanup steps. Quick Reference (CLI examples) Note: replace placeholders with real values from your environment.\n# Verify CLI profile identity aws sts get-caller-identity --profile migration # List EC2 instances and output JSON aws ec2 describe-instances --profile migration --output json \u0026gt; inventory-instances.json # Sync VM artifacts to S3 (stage for import) aws s3 sync ./vm-artifacts s3://fcj-migration-bucket/vm-artifacts --profile migration # Start VM Import task (example assumes IAM role and S3 staging prepared) aws ec2 import-image --description \u0026#34;FCJ VM import\u0026#34; --disk-containers file://containers.json --profile migration # Create a DMS replication instance (example) aws dms create-replication-instance --replication-instance-identifier fcj-dms --allocated-storage 50 --replication-instance-class dms.t3.medium --profile migration # Start a DMS task (assumes endpoints and replication instance exist) aws dms start-replication-task --replication-task-arn arn:aws:dms:... --start-replication-task-type start-replication --profile migration # Cleanup example: delete staging S3 objects aws s3 rm s3://fcj-migration-bucket/vm-artifacts --recursive --profile migration References Getting Started with the AWS CLI: https://000011.awsstudygroup.com/ Migrate to AWS overview: https://cloudjourney.awsstudygroup.com/2-migrate/ VM import/export workshop: https://000014.awsstudygroup.com/ Database Migration (DMS) workshop: https://000043.awsstudygroup.com/ If you\u0026rsquo;d like, I can:\nExpand each day with full step-by-step CLI commands and safe verification checks. Produce CloudFormation/Terraform snippets to automate parts of the migration pipeline (staging S3, IAM roles, DMS instance). Create a Vietnamese translation of this page. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Overview Week 12 is focused on deploying WordPress on AWS Cloud using EC2, RDS, Load Balancers, Auto Scaling, and CloudFront. The content follows the workshop \u0026ldquo;TRI·ªÇN KHAI WORDPRESS TR√äN AWS CLOUD\u0026rdquo; at https://000021.awsstudygroup.com/.\nObjectives Deploy a highly available WordPress site using EC2 instances in public subnets and an RDS database in private subnets. Configure an Application Load Balancer, Target Group, and Auto Scaling Group for the web tier. Use Amazon RDS (Multi‚ÄëAZ) for database availability and implement snapshot/backup and restore procedures. (Optional) Configure Amazon CloudFront as CDN for improved global performance. Perform cleanup to remove resources and avoid charges. Prerequisites AWS account with permissions for EC2, RDS, ELB, Auto Scaling, CloudFront, and IAM. Basic knowledge of VPC, subnets, security groups, and SSH access to EC2 instances. awscli configured (optional) for running CLI examples. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Workshop overview \u0026amp; environment prep: read guide, confirm VPC/subnet strategy and IAM roles. 12/03/2025 12/03/2025 https://000021.awsstudygroup.com/1-introduce/, https://000021.awsstudygroup.com/2-prerequiste/ 2 Install WordPress on EC2: launch EC2 in public subnet, install web stack (NGINX/Apache + PHP), and configure WordPress files. 12/04/2025 12/04/2025 https://000021.awsstudygroup.com/3-installwordpressonec2/ 3 RDS setup: create an RDS instance in private subnet (Multi‚ÄëAZ optional), configure security groups, and connect WordPress to RDS. 12/05/2025 12/05/2025 https://000021.awsstudygroup.com/5-backupandrestore/ 4 Auto Scaling \u0026amp; Load Balancer: create Target Group, ALB, and Auto Scaling Group for web tier; verify session handling and health checks. 12/06/2025 12/06/2025 https://000021.awsstudygroup.com/4-asgforec2/ 5 Snapshot \u0026amp; Backup: create RDS snapshots, test restore procedure, and implement instance snapshots for web tier. 12/07/2025 12/07/2025 https://000021.awsstudygroup.com/5-backupandrestore/ 6 CloudFront (optional): create CloudFront distribution for the WordPress site, configure origin, and test cached responses. 12/08/2025 12/08/2025 https://000021.awsstudygroup.com/6-createcloudfront/ 7 Cleanup: delete EC2 instances, Auto Scaling Group, Target Group, ALB, RDS instances/snapshots (if test), CloudFront distribution, and other artifacts. 12/09/2025 12/09/2025 https://000021.awsstudygroup.com/7-cleanup/ Week 12 Achievements Deployed a WordPress site on EC2 connected to an RDS backend and verified end-to-end functionality. Configured an ALB and Auto Scaling Group to provide resilience and scaling for the web tier. Created and restored RDS snapshots to validate backup/restore procedures. (Optional) Deployed CloudFront distribution and observed improved content delivery. Cleaned up resources to avoid ongoing charges. Quick Reference (CLI examples) Note: replace placeholder values with those from your account.\n# Launch an EC2 instance (example) aws ec2 run-instances --image-id ami-0123456789abcdef0 --count 1 --instance-type t3.micro --key-name MyKey --security-group-ids sg-xxxx --subnet-id subnet-aaa # Create an RDS instance (MySQL example) aws rds create-db-instance --db-instance-identifier wordpress-db --allocated-storage 20 --db-instance-class db.t3.micro --engine mysql --master-username admin --master-user-password \u0026#39;P@ssw0rd!\u0026#39; --vpc-security-group-ids sg-xxxx --db-subnet-group-name my-db-subnet-group --multi-az # Create an application load balancer aws elbv2 create-load-balancer --name wordpress-alb --subnets subnet-aaa subnet-bbb --security-groups sg-xxxx # Create target group aws elbv2 create-target-group --name wordpress-targets --protocol HTTP --port 80 --vpc-id vpc-xxxxxxxx # Register instance to target group aws elbv2 register-targets --target-group-arn arn:aws:elasticloadbalancing:...:targetgroup/wordpress-targets/xxxxxxxx --targets Id=i-0123456789abcdef0 # Create Auto Scaling Group (example) aws autoscaling create-auto-scaling-group --auto-scaling-group-name wordpress-asg --launch-configuration-name my-launch-config --min-size 1 --max-size 3 --desired-capacity 1 --vpc-zone-identifier \u0026#34;subnet-aaa,subnet-bbb\u0026#34; # Create RDS snapshot aws rds create-db-snapshot --db-instance-identifier wordpress-db --db-snapshot-identifier wordpress-db-snap-1 # Create CloudFront distribution (example using S3 origin or ALB) aws cloudfront create-distribution --distribution-config file://cf-distribution.json # Cleanup examples (delete resources when finished) aws rds delete-db-instance --db-instance-identifier wordpress-db --skip-final-snapshot --delete-automated-backups aws elbv2 delete-load-balancer --load-balancer-arn arn:aws:elasticloadbalancing:...:loadbalancer/app/wordpress-alb/xxxxxxxx aws autoscaling delete-auto-scaling-group --auto-scaling-group-name wordpress-asg --force-delete References Deploy WordPress on AWS Cloud: https://000021.awsstudygroup.com/ Modules: Introduction, Preparatory Steps, Install WordPress on EC2, Auto Scaling for WordPress, Backup and Restore, CloudFront, Cleanup If you\u0026rsquo;d like, I can:\nExpand each day with precise step-by-step commands and verification checks. Provide a sample cf-distribution.json and dashboard.json for CloudFront and monitoring. Produce a Vietnamese translation of the page. "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]